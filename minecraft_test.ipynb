{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minecraft1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "interpreter": {
      "hash": "b64057e63add2b45b1ffc7eab9b09c8889b419c878e2fdf0d08f837f0fc857a7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikros98/Minecraft-settlement-GAN/blob/main/minecraft_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7LwZN1CZ-yr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e4c7dd-d30c-441e-8036-bb4ae37eb2fb"
      },
      "source": [
        "# code for GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/FProject')\n",
        "folder = '/content/gdrive/MyDrive/FProject/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn7NxPcARcPx",
        "outputId": "93a56d3f-7c4a-4f16-878f-fdb6d175c2a8"
      },
      "source": [
        "# auto reload\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "!pip install nbtlib\n",
        "!git clone https://ikros98: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@github.com/ikros98/Minecraft-settlement-GAN.git\n",
        "%cd Minecraft-settlement-GAN/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nbtlib\n",
            "  Downloading nbtlib-1.7.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16 in /usr/local/lib/python3.7/dist-packages (from nbtlib) (1.19.5)\n",
            "Installing collected packages: nbtlib\n",
            "Successfully installed nbtlib-1.7.0\n",
            "Cloning into 'Minecraft-settlement-GAN'...\n",
            "remote: Enumerating objects: 5815, done.\u001b[K\n",
            "remote: Counting objects: 100% (5815/5815), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5715/5715), done.\u001b[K\n",
            "remote: Total 5815 (delta 253), reused 5651 (delta 98), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (5815/5815), 21.31 MiB | 25.11 MiB/s, done.\n",
            "Resolving deltas: 100% (253/253), done.\n",
            "/content/Minecraft-settlement-GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKFD6ceJGW2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13bc281-8032-4642-f3ac-d57dfdc085d5"
      },
      "source": [
        "%cd Minecraft-settlement-GAN/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Minecraft-settlement-GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPrDfxso_EdK",
        "outputId": "c8c2f9bc-4680-4672-ce16-887382f19022"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything up-to-date\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLbL-0kQwJO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9058722d-cfbb-4215-c3fe-42d767d9d3ad"
      },
      "source": [
        "import SchematicTools\n",
        "from schematic import SchematicFile\n",
        "import numpy as np\n",
        "import SchematicTools\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# colab\n",
        "folder = '/content/Minecraft-settlement-GAN/'\n",
        "# local\n",
        "#folder = '/Users/ikros/Documents/GitHub/Minecraft-settlement-GAN/'\n",
        "\n",
        "# function to load the entire dataset\n",
        "def load_dataset():\n",
        "    l = []\n",
        "    for file in os.listdir(folder + \"dataset808016/\"):\n",
        "        settlement = SchematicTools.simplify(SchematicTools.loadArea(folder + \"dataset808016/\" + file))\n",
        "        l.append(settlement)\n",
        "    x = np.array(l)\n",
        "    return (x)\n",
        "\n",
        "X = load_dataset()\n",
        "X = 1 * X\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3], 1).astype(np.float32) #?? to fix float 32, maybe int\n",
        "print(np.count_nonzero(X[0] == 0))\n",
        "print(np.count_nonzero(X[0] == 1))\n",
        "X = np.where(X == 0, -1, X) # Input reshape for tanh. It should work\n",
        "print(\"ciao\")\n",
        "print(np.count_nonzero(X[0] == -1))\n",
        "print(np.count_nonzero(X[0] == 1))\n",
        "\n",
        "print(np.unique(X[0].flatten()))\n",
        "print(X.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97546\n",
            "4854\n",
            "ciao\n",
            "97546\n",
            "4854\n",
            "[-1.  1.]\n",
            "(2500, 16, 80, 80, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBKoSbs23tNQ"
      },
      "source": [
        "My prova"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Bb6y6Fc37T"
      },
      "source": [
        "# this cleans transforms all the cubes in concrete\n",
        "simpleWorlds = SchematicTools.simplify(SchematicTools.loadArea(folder + 'mcedit settlement generator/0.schematic'))\n",
        "#exporting after simpleWorlds became\n",
        "exportSchematic = SchematicFile(shape=simpleWorlds.shape)\n",
        "exportSchematic.blocks = simpleWorlds\n",
        "exportSchematic.save(folder + \"mcedit settlement generator/sampledExample.schematic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83-hsgU6nGL"
      },
      "source": [
        "My prova 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWzVepS46pY8"
      },
      "source": [
        "# it cleans the file from unnecessary blocks\n",
        "simpleWorlds = SchematicTools.simplify2(SchematicTools.loadArea(folder + 'mcedit settlement generator/0.schematic'))\n",
        "#exporting after simpleWorlds became\n",
        "exportSchematic = SchematicFile(shape=simpleWorlds.shape)\n",
        "exportSchematic.blocks = simpleWorlds\n",
        "exportSchematic.save(folder + \"mcedit settlement generator/sampledExample.schematic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83M4laLdwJPA"
      },
      "source": [
        "# cleaning all the schematics file from not useful \n",
        "for file in os.listdir(folder + \"dataset808020/\"):\n",
        "    settlement = SchematicTools.simplify2(SchematicTools.loadArea(folder + \"dataset808020/\" + file))\n",
        "    exportSchematic = SchematicFile(shape=settlement.shape)\n",
        "    exportSchematic.blocks = settlement\n",
        "    exportSchematic.save(folder + \"clean dataset/\" + file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI_U3paZwJPA"
      },
      "source": [
        "# creating a csv file with the cleaned settlements\n",
        "l = []\n",
        "for file in os.listdir(folder + \"clean dataset/\"):\n",
        "    settlement = SchematicTools.loadArea(folder + \"clean dataset/\" + file)\n",
        "    l.append(settlement.ravel())\n",
        "dataset = pd.DataFrame(l)\n",
        "dataset.to_csv('out.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kn6wlFP3tlg"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "# have to understand how to manage the 30 that becomes 28\n",
        "\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(5*20*20*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((5, 20, 20, 256)))\n",
        "    assert model.output_shape == (None, 5, 20, 20, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(128, (5, 5, 5), strides=(1, 1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 5, 20, 20, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(64, (5, 5, 5), strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 10, 40, 40, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(1, (5, 5, 5), strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 20, 80, 80, 1)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv3D(64, (5, 5, 5), strides=(2, 2, 2), padding='same',\n",
        "                                     input_shape=[20, 80, 80, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv3D(128, (5, 5, 5), strides=(2, 2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp19QaVekyet"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "import random \n",
        "\n",
        "def make_generator_model(sig = False):\n",
        "  output_size, half, forth, eighth, sixteenth = 80, 40, 20, 10, 5\n",
        "  output_size_h, half_h, forth_h, eighth_h, sixteenth_h = 16, 8, 4, 2, 1\n",
        "  gf_dim = 256 # Dimension of gen filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  model.add(layers.Dense(sixteenth_h*sixteenth*sixteenth*gf_dim, kernel_initializer=tf.random_normal_initializer(stddev=0.02), input_shape=(200,)))\n",
        "  model.add(layers.Reshape((sixteenth_h, sixteenth, sixteenth, gf_dim)))\n",
        "  assert model.output_shape == (None, 1, 5, 5, 256)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim, [eighth_h, eighth, eighth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 2, 10, 10, 256)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/2, [forth_h, forth, forth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim/2, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 4, 20, 20, 128)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/4, [forth_h, forth, forth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim/4, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 8, 40, 40, 64)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(1, [output_size_h, output_size, output_size], strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "  model.add(layers.Conv3DTranspose(1, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "  assert model.output_shape == (None, 16, 80, 80, 1)\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/8, [output_size_h, output_size, output_size], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  #model.add(layers.Reshape([output_size_h, output_size, output_size, gf_dim]))\n",
        "  #assert model.output_shape == (16, 80, 80, 256)\n",
        "  #model.add(layers.Reshape([output_size_h, output_size, output_size, int(gf_dim/8)]))\n",
        "  #assert model.output_shape == (None, 16, 80, 80, 32)\n",
        "  #model.add(layers.Reshape([32, output_size_h, output_size, output_size]))\n",
        "  #assert model.output_shape == (16, 80, 80, 256)\n",
        "\n",
        "  \"\"\"\n",
        "  if sig: \n",
        "    model.add(layers.Activation('sigmoid'))\n",
        "  else:\n",
        "    model.add(layers.Activation('tanh'))\"\"\"\n",
        "  \n",
        "  return model\n",
        "\n",
        "def make_discriminator_model(sig = False, output_units = 1):\n",
        "  df_dim = 32 # Dimension of discrim filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv3D(df_dim, (4, 4, 4), strides=(2, 2, 2), input_shape=[16, 80, 80, 1],  activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*2, (4, 4, 4), strides=(2, 2, 2), activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*4, (4, 4, 4), strides=(2, 2, 2), activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*8, (4, 4, 4), strides=(2, 2, 2), activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  #model.add(layers.Flatten())\n",
        "\n",
        "  model.add(layers.Dense(output_units, kernel_initializer=tf.random_normal_initializer(stddev=0.02), input_dim=32))\n",
        "\n",
        "  if sig: \n",
        "    model.add(layers.Activation('sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5XSkcKF3BQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca00d6b-7163-4df5-cec4-74b5784e437b"
      },
      "source": [
        "#WGAN\n",
        "BUFFER_SIZE = 2496#2500\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "noise_dim = 200\n",
        "gp_weight=10.0\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "def gradient_penalty(batch_size, discriminator, real_images, fake_images):\n",
        "  \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "  This loss is calculated on an interpolated image\n",
        "  and added to the discriminator loss.\n",
        "  \"\"\"\n",
        "  # Get the interpolated image\n",
        "  alpha = tf.random.normal([batch_size, 1, 1, 1, 1], 0.0, 1.0)\n",
        "  diff = fake_images - real_images\n",
        "  interpolated = real_images + alpha * diff\n",
        "\n",
        "  with tf.GradientTape() as gp_tape:\n",
        "      gp_tape.watch(interpolated)\n",
        "      # 1. Get the discriminator output for this interpolated image.\n",
        "      pred = discriminator(interpolated, training=True)\n",
        "\n",
        "  # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "  grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "  # 3. Calculate the norm of the gradients.\n",
        "  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "  gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "  return gp\n",
        "\n",
        "# Define the loss functions for the discriminator,\n",
        "# which should be (fake_loss - real_loss).\n",
        "# We will add the gradient penalty later to this loss function.\n",
        "def discriminator_loss(real_img, fake_img):\n",
        "    real_loss = tf.reduce_mean(real_img)\n",
        "    fake_loss = tf.reduce_mean(fake_img)\n",
        "    return fake_loss - real_loss\n",
        "\n",
        "\n",
        "# Define the loss functions for the generator.\n",
        "def generator_loss(fake_img):\n",
        "    return -tf.reduce_mean(fake_img)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "checkpoint_dir = '/content/gdrive/MyDrive/FProject/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(real_images):\n",
        "  if isinstance(real_images, tuple):\n",
        "      real_images = real_images[0]\n",
        "\n",
        "  # Get the batch size\n",
        "  batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "  # For each batch, we are going to perform the\n",
        "  # following steps as laid out in the original paper:\n",
        "  # 1. Train the generator and get the generator loss\n",
        "  # 2. Train the discriminator and get the discriminator loss\n",
        "  # 3. Calculate the gradient penalty\n",
        "  # 4. Multiply this gradient penalty with a constant weight factor\n",
        "  # 5. Add the gradient penalty to the discriminator loss\n",
        "  # 6. Return the generator and discriminator losses as a loss dictionary\n",
        "\n",
        "  # Train the discriminator first. The original paper recommends training\n",
        "  # the discriminator for `x` more steps (typically 5) as compared to\n",
        "  # one step of the generator. Here we will train it for 3 extra steps\n",
        "  # as compared to 5 to reduce the training time.\n",
        "  for i in range(3):\n",
        "      # Get the latent vector\n",
        "      random_latent_vectors = tf.random.normal(\n",
        "          shape=(batch_size, noise_dim)\n",
        "      )\n",
        "      with tf.GradientTape() as tape:\n",
        "          # Generate fake images from the latent vector\n",
        "          fake_images = generator(random_latent_vectors, training=True)\n",
        "          # Get the logits for the fake images\n",
        "          fake_logits = discriminator(fake_images, training=True)\n",
        "          # Get the logits for the real images\n",
        "          real_logits = discriminator(real_images, training=True)\n",
        "\n",
        "          # Calculate the discriminator loss using the fake and real image logits\n",
        "          d_cost = discriminator_loss(real_img=real_logits, fake_img=fake_logits)\n",
        "          # Calculate the gradient penalty\n",
        "          gp = gradient_penalty(batch_size, discriminator, real_images, fake_images)\n",
        "          # Add the gradient penalty to the original discriminator loss\n",
        "          d_loss = d_cost + gp * gp_weight\n",
        "\n",
        "      # Get the gradients w.r.t the discriminator loss\n",
        "      d_gradient = tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "      # Update the weights of the discriminator using the discriminator optimizer\n",
        "      discriminator_optimizer.apply_gradients(\n",
        "          zip(d_gradient, discriminator.trainable_variables)\n",
        "      )\n",
        "\n",
        "  # Train the generator\n",
        "  # Get the latent vector\n",
        "  random_latent_vectors = tf.random.normal(shape=(batch_size, noise_dim))\n",
        "  with tf.GradientTape() as tape:\n",
        "      # Generate fake images using the generator\n",
        "      generated_images = generator(random_latent_vectors, training=True)\n",
        "      # Get the discriminator logits for fake images\n",
        "      gen_img_logits = discriminator(generated_images, training=True)\n",
        "      # Calculate the generator loss\n",
        "      g_loss = generator_loss(gen_img_logits)\n",
        "\n",
        "  # Get the gradients w.r.t the generator loss\n",
        "  gen_gradient = tape.gradient(g_loss, generator.trainable_variables)\n",
        "  # Update the weights of the generator using the generator optimizer\n",
        "  generator_optimizer.apply_gradients(\n",
        "      zip(gen_gradient, generator.trainable_variables)\n",
        "  )\n",
        "  ### return {\"d_loss\": d_loss, \"g_loss\": g_loss}    \n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    print()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Time for epoch 1 is 108.69861888885498 sec\n",
            "\n",
            "Time for epoch 2 is 69.69897413253784 sec\n",
            "\n",
            "Time for epoch 3 is 70.81933808326721 sec\n",
            "\n",
            "Time for epoch 4 is 70.87185978889465 sec\n",
            "\n",
            "Time for epoch 5 is 70.93909668922424 sec\n",
            "\n",
            "Time for epoch 6 is 70.9078962802887 sec\n",
            "\n",
            "Time for epoch 7 is 70.9492826461792 sec\n",
            "\n",
            "Time for epoch 8 is 70.88805818557739 sec\n",
            "\n",
            "Time for epoch 9 is 70.94354009628296 sec\n",
            "\n",
            "Time for epoch 10 is 70.94531679153442 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdwZXhTG3jYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1c909c-8376-4222-b0d2-fd161ca8276e"
      },
      "source": [
        "# DCGAN\n",
        "\n",
        "BUFFER_SIZE = 2496#2500\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "noise_dim = 200\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  return cross_entropy(real_output, fake_output)\n",
        "    #real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    #fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    #total_loss = real_loss + fake_loss\n",
        "    #return total_loss\n",
        "\n",
        "def generator_loss(label, output):\n",
        "    return cross_entropy(label, output)\n",
        "    #return -tf.reduce_mean(tf.math.log(fake_output))\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0025, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.00005, beta_1=0.5)\n",
        "\n",
        "checkpoint_dir = '/content/gdrive/MyDrive/FProject/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    # noise vector sampled from normal distribution\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    # Train Discriminator with real labels\n",
        "    with tf.GradientTape() as disc_tape1:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        \n",
        "        real_output = discriminator(images, training=True)\n",
        "        real_targets = tf.ones_like(real_output)\n",
        "        disc_loss1 = discriminator_loss(real_targets, real_output)\n",
        "        \n",
        "    # gradient calculation for discriminator for real labels    \n",
        "    gradients_of_disc1 = disc_tape1.gradient(disc_loss1, discriminator.trainable_variables)\n",
        "    \n",
        "    # parameters optimization for discriminator for real labels   \n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc1,\\\n",
        "    discriminator.trainable_variables))\n",
        "    \n",
        "    # Train Discriminator with fake labels\n",
        "    with tf.GradientTape() as disc_tape2:\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        fake_targets = tf.zeros_like(fake_output)\n",
        "        disc_loss2 = discriminator_loss(fake_targets, fake_output)\n",
        "    # gradient calculation for discriminator for fake labels \n",
        "    gradients_of_disc2 = disc_tape2.gradient(disc_loss2, discriminator.trainable_variables)\n",
        "    \n",
        "    \n",
        "    # parameters optimization for discriminator for fake labels        \n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc2,\\\n",
        "    discriminator.trainable_variables))\n",
        "    \n",
        "    # Train Generator with real labels\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        real_targets = tf.ones_like(fake_output)\n",
        "        gen_loss = generator_loss(real_targets, fake_output)\n",
        "\n",
        "    # gradient calculation for generator for real labels     \n",
        "    gradients_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    \n",
        "    # parameters optimization for generator for real labels\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_gen,\\\n",
        "    generator.trainable_variables))    \n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    print()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      if image_batch.numpy().shape[0] == 4:\n",
        "        print(\"shape error\")\n",
        "        #continue\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "shape error\n",
            "Time for epoch 1 is 29.518051147460938 sec\n",
            "\n",
            "shape error\n",
            "Time for epoch 2 is 26.133253574371338 sec\n",
            "\n",
            "shape error\n",
            "Time for epoch 3 is 25.583948850631714 sec\n",
            "\n",
            "shape error\n",
            "Time for epoch 4 is 25.890392065048218 sec\n",
            "\n",
            "shape error\n",
            "Time for epoch 5 is 26.08562707901001 sec\n",
            "\n",
            "shape error\n",
            "Time for epoch 6 is 25.859896421432495 sec\n",
            "\n",
            "shape error\n",
            "Time for epoch 7 is 25.820560216903687 sec\n",
            "\n",
            "shape error\n",
            "Time for epoch 8 is 25.85247278213501 sec\n",
            "\n",
            "shape error\n",
            "Time for epoch 9 is 25.851152181625366 sec\n",
            "\n",
            "shape error\n",
            "Time for epoch 10 is 25.86657452583313 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8an_6J9cXQ4"
      },
      "source": [
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Load our pre-trained model\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)).expect_partial()\n",
        "\n",
        "print(generator.summary())\n",
        "print(discriminator.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3hackEJpqOB",
        "outputId": "f8340e85-f893-42d5-d61f-3c8d1100dd6c"
      },
      "source": [
        "structureCount = 5\n",
        "noise_dim = 200\n",
        "\n",
        "predictions = generator(tf.random.normal([structureCount, noise_dim]), training=False)\n",
        "#print(type(predictions[0])) Eager tensor\n",
        "#predictions = predictions * .5 + .5\n",
        "predictions = predictions.numpy()\n",
        "print(np.count_nonzero(predictions[0] < 0))\n",
        "print(np.count_nonzero(predictions[0] > 0))\n",
        "predictions.shape = [structureCount, 16, 80, 80]\n",
        "for i in range(structureCount):\n",
        "  structure = predictions[i, :, :, :]\n",
        "  #structure = np.where(structure >= 0.0015888, 1, 0)\n",
        "  structure = np.where(structure >= 0, 1, 0)\n",
        "  print(\"generated %s\" % str(structure.shape))\n",
        "  print(np.count_nonzero(structure == 0))\n",
        "  print(np.count_nonzero(structure == 1))\n",
        "  exportSchematic = SchematicFile(shape=structure.shape)\n",
        "  exportSchematic.blocks = structure\n",
        "  exportSchematic.save(folder + str(i) + \".schematic\")\n",
        "  print(\"exported to \" + \"data/generatedExample.schematic\")\n",
        "  print(np.unique(structure.flatten()))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96602\n",
            "5798\n",
            "generated (16, 80, 80)\n",
            "96602\n",
            "5798\n",
            "exported to data/generatedExample.schematic\n",
            "[0 1]\n",
            "generated (16, 80, 80)\n",
            "93737\n",
            "8663\n",
            "exported to data/generatedExample.schematic\n",
            "[0 1]\n",
            "generated (16, 80, 80)\n",
            "94312\n",
            "8088\n",
            "exported to data/generatedExample.schematic\n",
            "[0 1]\n",
            "generated (16, 80, 80)\n",
            "93720\n",
            "8680\n",
            "exported to data/generatedExample.schematic\n",
            "[0 1]\n",
            "generated (16, 80, 80)\n",
            "92756\n",
            "9644\n",
            "exported to data/generatedExample.schematic\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}