{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minecraft1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "interpreter": {
      "hash": "b64057e63add2b45b1ffc7eab9b09c8889b419c878e2fdf0d08f837f0fc857a7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikros98/Minecraft-settlement-GAN/blob/main/minecraft_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7LwZN1CZ-yr"
      },
      "source": [
        "# code for GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/FProject')\n",
        "folder = '/content/gdrive/MyDrive/FProject/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARKoPNG5wJO8"
      },
      "source": [
        "# auto reload\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn7NxPcARcPx",
        "outputId": "7877bb4e-1c91-4b08-f2ca-ae0ac7eb856e"
      },
      "source": [
        "!pip install nbtlib\n",
        "!git clone https://ikros98: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@github.com/ikros98/Minecraft-settlement-GAN.git\n",
        "%cd Minecraft-settlement-GAN/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nbtlib\n",
            "  Downloading nbtlib-1.7.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16 in /usr/local/lib/python3.7/dist-packages (from nbtlib) (1.19.5)\n",
            "Installing collected packages: nbtlib\n",
            "Successfully installed nbtlib-1.7.0\n",
            "Cloning into 'Minecraft-settlement-GAN'...\n",
            "remote: Enumerating objects: 5077, done.\u001b[K\n",
            "remote: Counting objects: 100% (5077/5077), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4979/4979), done.\u001b[K\n",
            "remote: Total 5077 (delta 246), reused 4921 (delta 96), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (5077/5077), 19.20 MiB | 26.01 MiB/s, done.\n",
            "Resolving deltas: 100% (246/246), done.\n",
            "/content/Minecraft-settlement-GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKFD6ceJGW2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13bc281-8032-4642-f3ac-d57dfdc085d5"
      },
      "source": [
        "%cd Minecraft-settlement-GAN/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Minecraft-settlement-GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPrDfxso_EdK",
        "outputId": "c8c2f9bc-4680-4672-ce16-887382f19022"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything up-to-date\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EefohfYwJO-"
      },
      "source": [
        "# colab\n",
        "folder = '/content/Minecraft-settlement-GAN/'\n",
        "# local\n",
        "#folder = '/Users/ikros/Documents/GitHub/Minecraft-settlement-GAN/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLbL-0kQwJO_"
      },
      "source": [
        "import SchematicTools\n",
        "from schematic import SchematicFile\n",
        "import numpy as np\n",
        "import SchematicTools\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBKoSbs23tNQ"
      },
      "source": [
        "My prova"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Bb6y6Fc37T"
      },
      "source": [
        "# this cleans transforms all the cubes in concrete\n",
        "simpleWorlds = SchematicTools.simplify(SchematicTools.loadArea(folder + 'mcedit settlement generator/0.schematic'))\n",
        "#exporting after simpleWorlds became\n",
        "exportSchematic = SchematicFile(shape=simpleWorlds.shape)\n",
        "exportSchematic.blocks = simpleWorlds\n",
        "exportSchematic.save(folder + \"mcedit settlement generator/sampledExample.schematic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83-hsgU6nGL"
      },
      "source": [
        "My prova 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWzVepS46pY8"
      },
      "source": [
        "# it cleans the file from unnecessary blocks\n",
        "simpleWorlds = SchematicTools.simplify2(SchematicTools.loadArea(folder + 'mcedit settlement generator/0.schematic'))\n",
        "#exporting after simpleWorlds became\n",
        "exportSchematic = SchematicFile(shape=simpleWorlds.shape)\n",
        "exportSchematic.blocks = simpleWorlds\n",
        "exportSchematic.save(folder + \"mcedit settlement generator/sampledExample.schematic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83M4laLdwJPA"
      },
      "source": [
        "# cleaning all the schematics file from not useful \n",
        "for file in os.listdir(folder + \"dataset808020/\"):\n",
        "    settlement = SchematicTools.simplify2(SchematicTools.loadArea(folder + \"dataset808020/\" + file))\n",
        "    exportSchematic = SchematicFile(shape=settlement.shape)\n",
        "    exportSchematic.blocks = settlement\n",
        "    exportSchematic.save(folder + \"clean dataset/\" + file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI_U3paZwJPA"
      },
      "source": [
        "# creating a csv file with the cleaned settlements\n",
        "l = []\n",
        "for file in os.listdir(folder + \"clean dataset/\"):\n",
        "    settlement = SchematicTools.loadArea(folder + \"clean dataset/\" + file)\n",
        "    l.append(settlement.ravel())\n",
        "dataset = pd.DataFrame(l)\n",
        "dataset.to_csv('out.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0kBe5wxwJPB"
      },
      "source": [
        "# function to load the entire dataset\n",
        "def load_dataset():\n",
        "    l = []\n",
        "    for file in os.listdir(folder + \"dataset808016/\"):\n",
        "        settlement = SchematicTools.simplify(SchematicTools.loadArea(folder + \"dataset808016/\" + file))\n",
        "        l.append(settlement)\n",
        "    x = np.array(l)\n",
        "    return (x)\n",
        "\n",
        "X = load_dataset()\n",
        "X = 1 * X\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3], 1).astype(np.float32) #?? to fix float 32, maybe int\n",
        "X = np.where(X == 0, -1, X) # Input reshape for tanh. It should work"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuCle-RVE34l",
        "outputId": "d2e8d00f-0563-4a9a-9d50-dba039a1be59"
      },
      "source": [
        "type(X[0][0][0][0][0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.bool_"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2yf8BsXHbDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d9d8ba-d388-45c8-e82f-f069397c3c3d"
      },
      "source": [
        "print(np.unique(X[0].flatten()))\n",
        "print(X.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.  1.]\n",
            "(1772, 16, 80, 80, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-D6EnwgGfmO"
      },
      "source": [
        "# normalizing the images to [-1, 1]\n",
        "def normalise(settlement):\n",
        "  settlement = tf.cast(settlement, tf.float32)\n",
        "  settlement = (settlement / 69.5) - 1\n",
        "  return settlement\n",
        "\n",
        "prova = normalise(X[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzDn14IEH_CW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c0b2067-a3b4-4aac-8a97-758ecb12da14"
      },
      "source": [
        "np.unique(prova.numpy().flatten())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kn6wlFP3tlg"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "# have to understand how to manage the 30 that becomes 28\n",
        "\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(5*20*20*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((5, 20, 20, 256)))\n",
        "    assert model.output_shape == (None, 5, 20, 20, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(128, (5, 5, 5), strides=(1, 1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 5, 20, 20, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(64, (5, 5, 5), strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 10, 40, 40, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(1, (5, 5, 5), strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 20, 80, 80, 1)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv3D(64, (5, 5, 5), strides=(2, 2, 2), padding='same',\n",
        "                                     input_shape=[20, 80, 80, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv3D(128, (5, 5, 5), strides=(2, 2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5XSkcKF3BQy"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "generated_image = generator(noise, training=False)\n",
        "\"\"\"\n",
        "#plt.imshow(generated_image[0, :, :, :, 0], cmap='gray')\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "def make_ax(grid=False):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca(projection='3d')\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"y\")\n",
        "    ax.set_zlabel(\"z\")#\n",
        "    ax.grid(grid)\n",
        "    return ax\n",
        "\n",
        "ax = make_ax(True)\n",
        "ax.voxels(generated_image[0, :, :, :, 0], edgecolors='gray', shade=False)\n",
        "plt.show()\"\"\"\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "EPOCHS = 10\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    #clear_output(wait=True)\n",
        "    #generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "\n",
        "  preview = np.empty((0, 80 * example_grid,3), np.uint8)\n",
        "  for i in range(example_grid):\n",
        "      row = np.empty((80, 0, 3), np.uint8)\n",
        "      for j in range(example_grid):\n",
        "          imageR = predictions[i*example_grid + j, :, :, :, 0] * 127.5 + 127.5\n",
        "          imageG = np.average(imageR, axis=0)\n",
        "          image = np.stack((imageR[0], imageG, imageG), axis=2)\n",
        "          row = np.hstack((row, image))\n",
        "      preview = np.vstack((preview, row))\n",
        "\n",
        "  outputimage = PIL.Image.fromarray(preview.astype(np.uint8))\n",
        "  outputimage.save('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  return outputimage\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp19QaVekyet"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "import random \n",
        "\n",
        "def make_generator_model(sig = False):\n",
        "  output_size, half, forth, eighth, sixteenth = 80, 40, 20, 10, 5\n",
        "  output_size_h, half_h, forth_h, eighth_h, sixteenth_h = 16, 8, 4, 2, 1\n",
        "  gf_dim = 256 # Dimension of gen filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  model.add(layers.Dense(sixteenth_h*sixteenth*sixteenth*gf_dim, kernel_initializer=tf.random_normal_initializer(stddev=0.02), input_shape=(200,)))\n",
        "  model.add(layers.Reshape((sixteenth_h, sixteenth, sixteenth, gf_dim)))\n",
        "  assert model.output_shape == (None, 1, 5, 5, 256)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim, [eighth_h, eighth, eighth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 2, 10, 10, 256)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/2, [forth_h, forth, forth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim/2, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 4, 20, 20, 128)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/4, [forth_h, forth, forth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim/4, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 8, 40, 40, 64)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(1, [output_size_h, output_size, output_size], strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "  model.add(layers.Conv3DTranspose(1, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "  assert model.output_shape == (None, 16, 80, 80, 1)\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/8, [output_size_h, output_size, output_size], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  #model.add(layers.Reshape([output_size_h, output_size, output_size, gf_dim]))\n",
        "  #assert model.output_shape == (16, 80, 80, 256)\n",
        "  #model.add(layers.Reshape([output_size_h, output_size, output_size, int(gf_dim/8)]))\n",
        "  #assert model.output_shape == (None, 16, 80, 80, 32)\n",
        "  #model.add(layers.Reshape([32, output_size_h, output_size, output_size]))\n",
        "  #assert model.output_shape == (16, 80, 80, 256)\n",
        "\n",
        "  \"\"\"\n",
        "  if sig: \n",
        "    model.add(layers.Activation('sigmoid'))\n",
        "  else:\n",
        "    model.add(layers.Activation('tanh'))\"\"\"\n",
        "  \n",
        "  return model\n",
        "\n",
        "def make_discriminator_model(sig = False, output_units = 1):\n",
        "  df_dim = 32 # Dimension of discrim filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv3D(df_dim, (4, 4, 4), strides=(2, 2, 2), input_shape=[16, 80, 80, 1],  activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*2, (4, 4, 4), strides=(2, 2, 2), activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*4, (4, 4, 4), strides=(2, 2, 2), activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*8, (4, 4, 4), strides=(2, 2, 2), activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  #model.add(layers.Flatten())\n",
        "\n",
        "  model.add(layers.Dense(output_units, kernel_initializer=tf.random_normal_initializer(stddev=0.02), input_dim=32))\n",
        "\n",
        "  if sig: \n",
        "    model.add(layers.Activation('sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdwZXhTG3jYE",
        "outputId": "41801fff-f543-4ce7-feb4-e3b6002ef667"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "BUFFER_SIZE = 1772\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 200\n",
        "noise_dim = 200\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    #return -tf.reduce_mean(tf.math.log(real_output) + tf.math.log(1. - fake_output))\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return -tf.reduce_mean(tf.math.log(fake_output))\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0025, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.00005, beta_1=0.5)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim], 0, 1)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_3d = generator(noise, training=True)\n",
        "      #generated_3d = tf.sign ( tf.sign(generated_3d) + 0.1 )\n",
        "      #generated_3d = tf.cast(generated_3d, tf.bool)\n",
        "      #generated_3d = tf.cast(generated_3d, tf.float32)\n",
        "      #prova = tf.identity(generated_3d)\n",
        "      #prova = tf.make_ndarray(prova)\n",
        "      #print(type(generated_3d[0][0][0][0]))\n",
        "      #print(np.unique(prova[0].flatten()))\n",
        "      print(generated_3d.shape)\n",
        "\n",
        "      #print(type(images[0][0][0][0]))\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_3d, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    #clear_output(wait=True)\n",
        "    #generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 16, 80, 80, 1)\n",
            "(32, 16, 80, 80, 1)\n",
            "(32, 16, 80, 80, 1)\n",
            "Time for epoch 1 is 21.542821884155273 sec\n",
            "Time for epoch 2 is 14.692021131515503 sec\n",
            "Time for epoch 3 is 14.817004680633545 sec\n",
            "Time for epoch 4 is 15.129055500030518 sec\n",
            "Time for epoch 5 is 15.10999321937561 sec\n",
            "Time for epoch 6 is 14.851027488708496 sec\n",
            "Time for epoch 7 is 14.652400016784668 sec\n",
            "Time for epoch 8 is 14.627325296401978 sec\n",
            "Time for epoch 9 is 14.761613130569458 sec\n",
            "Time for epoch 10 is 14.895858764648438 sec\n",
            "Time for epoch 11 is 14.851861238479614 sec\n",
            "Time for epoch 12 is 14.759517908096313 sec\n",
            "Time for epoch 13 is 14.729820251464844 sec\n",
            "Time for epoch 14 is 14.75278902053833 sec\n",
            "Time for epoch 15 is 14.790580987930298 sec\n",
            "Time for epoch 16 is 14.754884243011475 sec\n",
            "Time for epoch 17 is 14.778809785842896 sec\n",
            "Time for epoch 18 is 14.758686780929565 sec\n",
            "Time for epoch 19 is 14.789524555206299 sec\n",
            "Time for epoch 20 is 15.200600147247314 sec\n",
            "Time for epoch 21 is 14.782969236373901 sec\n",
            "Time for epoch 22 is 14.719228267669678 sec\n",
            "Time for epoch 23 is 14.734575986862183 sec\n",
            "Time for epoch 24 is 14.721737623214722 sec\n",
            "Time for epoch 25 is 14.767065525054932 sec\n",
            "Time for epoch 26 is 14.803905725479126 sec\n",
            "Time for epoch 27 is 14.776770830154419 sec\n",
            "Time for epoch 28 is 14.773481607437134 sec\n",
            "Time for epoch 29 is 14.74774980545044 sec\n",
            "Time for epoch 30 is 14.779658794403076 sec\n",
            "Time for epoch 31 is 14.757639408111572 sec\n",
            "Time for epoch 32 is 14.757745265960693 sec\n",
            "Time for epoch 33 is 14.7539381980896 sec\n",
            "Time for epoch 34 is 14.796355247497559 sec\n",
            "Time for epoch 35 is 14.79927682876587 sec\n",
            "Time for epoch 36 is 14.76129937171936 sec\n",
            "Time for epoch 37 is 14.731258630752563 sec\n",
            "Time for epoch 38 is 14.712844848632812 sec\n",
            "Time for epoch 39 is 14.745303392410278 sec\n",
            "Time for epoch 40 is 15.159047842025757 sec\n",
            "Time for epoch 41 is 14.84792447090149 sec\n",
            "Time for epoch 42 is 14.737697124481201 sec\n",
            "Time for epoch 43 is 14.711640119552612 sec\n",
            "Time for epoch 44 is 14.726020097732544 sec\n",
            "Time for epoch 45 is 14.789227962493896 sec\n",
            "Time for epoch 46 is 14.782135009765625 sec\n",
            "Time for epoch 47 is 14.728983163833618 sec\n",
            "Time for epoch 48 is 14.72799277305603 sec\n",
            "Time for epoch 49 is 14.736231327056885 sec\n",
            "Time for epoch 50 is 14.738970518112183 sec\n",
            "Time for epoch 51 is 14.715121746063232 sec\n",
            "Time for epoch 52 is 14.692229270935059 sec\n",
            "Time for epoch 53 is 14.697474718093872 sec\n",
            "Time for epoch 54 is 14.733741283416748 sec\n",
            "Time for epoch 55 is 14.77250862121582 sec\n",
            "Time for epoch 56 is 14.719191312789917 sec\n",
            "Time for epoch 57 is 14.702570915222168 sec\n",
            "Time for epoch 58 is 14.717522382736206 sec\n",
            "Time for epoch 59 is 14.759427547454834 sec\n",
            "Time for epoch 60 is 15.187735795974731 sec\n",
            "Time for epoch 61 is 14.790361166000366 sec\n",
            "Time for epoch 62 is 14.77095890045166 sec\n",
            "Time for epoch 63 is 14.728151798248291 sec\n",
            "Time for epoch 64 is 14.67135238647461 sec\n",
            "Time for epoch 65 is 14.694268226623535 sec\n",
            "Time for epoch 66 is 14.707864761352539 sec\n",
            "Time for epoch 67 is 14.695993661880493 sec\n",
            "Time for epoch 68 is 14.706222772598267 sec\n",
            "Time for epoch 69 is 14.725477457046509 sec\n",
            "Time for epoch 70 is 14.766245603561401 sec\n",
            "Time for epoch 71 is 14.756458282470703 sec\n",
            "Time for epoch 72 is 14.692776679992676 sec\n",
            "Time for epoch 73 is 14.696128845214844 sec\n",
            "Time for epoch 74 is 14.779757499694824 sec\n",
            "Time for epoch 75 is 14.71060299873352 sec\n",
            "Time for epoch 76 is 14.661935567855835 sec\n",
            "Time for epoch 77 is 14.688863277435303 sec\n",
            "Time for epoch 78 is 14.66890287399292 sec\n",
            "Time for epoch 79 is 14.733566284179688 sec\n",
            "Time for epoch 80 is 15.131636142730713 sec\n",
            "Time for epoch 81 is 14.718119859695435 sec\n",
            "Time for epoch 82 is 14.670433044433594 sec\n",
            "Time for epoch 83 is 14.72498893737793 sec\n",
            "Time for epoch 84 is 14.757168292999268 sec\n",
            "Time for epoch 85 is 14.716477870941162 sec\n",
            "Time for epoch 86 is 14.7318754196167 sec\n",
            "Time for epoch 87 is 14.768432855606079 sec\n",
            "Time for epoch 88 is 14.673908710479736 sec\n",
            "Time for epoch 89 is 14.655542612075806 sec\n",
            "Time for epoch 90 is 14.68207597732544 sec\n",
            "Time for epoch 91 is 14.701460599899292 sec\n",
            "Time for epoch 92 is 14.65747332572937 sec\n",
            "Time for epoch 93 is 14.659415245056152 sec\n",
            "Time for epoch 94 is 14.679973840713501 sec\n",
            "Time for epoch 95 is 14.73557448387146 sec\n",
            "Time for epoch 96 is 14.723844528198242 sec\n",
            "Time for epoch 97 is 14.705140113830566 sec\n",
            "Time for epoch 98 is 14.676735162734985 sec\n",
            "Time for epoch 99 is 14.690357208251953 sec\n",
            "Time for epoch 100 is 15.131431341171265 sec\n",
            "Time for epoch 101 is 14.665322065353394 sec\n",
            "Time for epoch 102 is 14.672214031219482 sec\n",
            "Time for epoch 103 is 14.689970254898071 sec\n",
            "Time for epoch 104 is 14.757507562637329 sec\n",
            "Time for epoch 105 is 14.719779253005981 sec\n",
            "Time for epoch 106 is 14.750471115112305 sec\n",
            "Time for epoch 107 is 14.737483501434326 sec\n",
            "Time for epoch 108 is 14.699012279510498 sec\n",
            "Time for epoch 109 is 14.681762456893921 sec\n",
            "Time for epoch 110 is 14.723981380462646 sec\n",
            "Time for epoch 111 is 14.733015537261963 sec\n",
            "Time for epoch 112 is 14.682408571243286 sec\n",
            "Time for epoch 113 is 14.7178053855896 sec\n",
            "Time for epoch 114 is 14.7637357711792 sec\n",
            "Time for epoch 115 is 14.752909421920776 sec\n",
            "Time for epoch 116 is 14.674861192703247 sec\n",
            "Time for epoch 117 is 14.677173614501953 sec\n",
            "Time for epoch 118 is 14.642795085906982 sec\n",
            "Time for epoch 119 is 14.696397304534912 sec\n",
            "Time for epoch 120 is 15.112173080444336 sec\n",
            "Time for epoch 121 is 20.87802815437317 sec\n",
            "Time for epoch 122 is 14.670328140258789 sec\n",
            "Time for epoch 123 is 14.944957494735718 sec\n",
            "Time for epoch 124 is 14.820327758789062 sec\n",
            "Time for epoch 125 is 14.664765357971191 sec\n",
            "Time for epoch 126 is 14.582827806472778 sec\n",
            "Time for epoch 127 is 14.587255001068115 sec\n",
            "Time for epoch 128 is 14.731302738189697 sec\n",
            "Time for epoch 129 is 14.788278579711914 sec\n",
            "Time for epoch 130 is 14.760817527770996 sec\n",
            "Time for epoch 131 is 14.705662965774536 sec\n",
            "Time for epoch 132 is 14.627245664596558 sec\n",
            "Time for epoch 133 is 14.644069910049438 sec\n",
            "Time for epoch 134 is 14.67053508758545 sec\n",
            "Time for epoch 135 is 14.732805728912354 sec\n",
            "Time for epoch 136 is 14.69859504699707 sec\n",
            "Time for epoch 137 is 14.729438781738281 sec\n",
            "Time for epoch 138 is 14.709157705307007 sec\n",
            "Time for epoch 139 is 14.71825361251831 sec\n",
            "Time for epoch 140 is 15.109298706054688 sec\n",
            "Time for epoch 141 is 14.665804147720337 sec\n",
            "Time for epoch 142 is 14.623612642288208 sec\n",
            "Time for epoch 143 is 14.69326901435852 sec\n",
            "Time for epoch 144 is 14.746315002441406 sec\n",
            "Time for epoch 145 is 14.714547157287598 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "v3hackEJpqOB",
        "outputId": "c4ce7d28-70dc-4ed2-80d1-7ef05b6a1392"
      },
      "source": [
        "structureCount = 5\n",
        "noise_dim = 200\n",
        "\n",
        "predictions = generator(tf.random.normal([structureCount, noise_dim]), training=False)\n",
        "predictions = predictions * .5 + .5\n",
        "#predictions = tf.cast(predictions, tf.int32)\n",
        "predictions = predictions.numpy().astype(np.int32)\n",
        "predictions.shape = [structureCount, 16, 80, 80]\n",
        "for i in range(structureCount):\n",
        "  structure = predictions[i, :, :, :]\n",
        "  structure = np.where(structure >= 0.5, 1, 0)\n",
        "  print(\"generated %s\" % str(structure.shape))\n",
        "  exportSchematic = SchematicFile(shape=structure.shape)\n",
        "  exportSchematic.blocks = structure\n",
        "  exportSchematic.save(folder + str(i) + \".schematic\")\n",
        "  print(\"exported to \" + \"data/generatedExample.schematic\")\n",
        "  print(np.unique(structure.flatten()))\n",
        "\"\"\"\n",
        "#export a grid of generated structures.\n",
        "world = np.empty((B3, (B3 + EXPORTSPACING) * EXPORTGRID, 0), np.uint8)\n",
        "for i in range(structureCount):\n",
        "    row = np.empty((B3, 0, B3), np.uint8)\n",
        "    for j in range(EXPORTGRID):\n",
        "        structure = predictions[i*EXPORTGRID + j, :, :, :]\n",
        "        structure = np.concatenate((structure, \n",
        "              np.zeros((B3, EXPORTSPACING, B3), \n",
        "              dtype=np.uint8)), axis=1)\n",
        "        row = np.concatenate((row, structure), axis=1)\n",
        "    world = np.concatenate((world, row), axis=2)\n",
        "    world = np.concatenate((world, \n",
        "              np.zeros((B3, (B3 + EXPORTSPACING) * EXPORTGRID, EXPORTSPACING), \n",
        "              dtype=np.uint8)), axis=2)\n",
        "    \n",
        "#export a grid of generated structures.\n",
        "world = np.empty((B3, (B3 + EXPORTSPACING) * EXPORTGRID, 0), np.uint8)\n",
        "for i in range(structureCount):\n",
        "    row = np.empty((B3, 0, B3), np.uint8)\n",
        "    structure = predictions[i, :, :, :]\n",
        "    structure = np.concatenate((structure, \n",
        "              np.zeros((B3, EXPORTSPACING, B3), \n",
        "              dtype=np.uint8)), axis=1)\n",
        "        row = np.concatenate((row, structure), axis=1)\n",
        "    world = np.concatenate((world, row), axis=2)\n",
        "    world = np.concatenate((world, \n",
        "              np.zeros((B3, (B3 + EXPORTSPACING) * EXPORTGRID, EXPORTSPACING), \n",
        "              dtype=np.uint8)), axis=2)\"\"\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated (16, 80, 80)\n",
            "exported to data/generatedExample.schematic\n",
            "[0]\n",
            "generated (16, 80, 80)\n",
            "exported to data/generatedExample.schematic\n",
            "[0]\n",
            "generated (16, 80, 80)\n",
            "exported to data/generatedExample.schematic\n",
            "[0]\n",
            "generated (16, 80, 80)\n",
            "exported to data/generatedExample.schematic\n",
            "[0]\n",
            "generated (16, 80, 80)\n",
            "exported to data/generatedExample.schematic\n",
            "[0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#export a grid of generated structures.\\nworld = np.empty((B3, (B3 + EXPORTSPACING) * EXPORTGRID, 0), np.uint8)\\nfor i in range(structureCount):\\n    row = np.empty((B3, 0, B3), np.uint8)\\n    for j in range(EXPORTGRID):\\n        structure = predictions[i*EXPORTGRID + j, :, :, :]\\n        structure = np.concatenate((structure, \\n              np.zeros((B3, EXPORTSPACING, B3), \\n              dtype=np.uint8)), axis=1)\\n        row = np.concatenate((row, structure), axis=1)\\n    world = np.concatenate((world, row), axis=2)\\n    world = np.concatenate((world, \\n              np.zeros((B3, (B3 + EXPORTSPACING) * EXPORTGRID, EXPORTSPACING), \\n              dtype=np.uint8)), axis=2)\\n    \\n#export a grid of generated structures.\\nworld = np.empty((B3, (B3 + EXPORTSPACING) * EXPORTGRID, 0), np.uint8)\\nfor i in range(structureCount):\\n    row = np.empty((B3, 0, B3), np.uint8)\\n    structure = predictions[i, :, :, :]\\n    structure = np.concatenate((structure, \\n              np.zeros((B3, EXPORTSPACING, B3), \\n              dtype=np.uint8)), axis=1)\\n        row = np.concatenate((row, structure), axis=1)\\n    world = np.concatenate((world, row), axis=2)\\n    world = np.concatenate((world, \\n              np.zeros((B3, (B3 + EXPORTSPACING) * EXPORTGRID, EXPORTSPACING), \\n              dtype=np.uint8)), axis=2)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "3-d0sEHK4YUN",
        "outputId": "55ac0d77-3b5f-43be-bfdd-d314f63ab167"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "BUFFER_SIZE = 500\n",
        "BATCH_SIZE = 10\n",
        "EPOCHS = 200\n",
        "noise_dim = 100\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([BATCH_SIZE, noise_dim], 0, 1)\n",
        "#generated_image = generator(noise, training=False)\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "#decision = discriminator(generated_image)\n",
        "#print(decision)\n",
        "\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0025, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.00005, beta_1=0.5)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "#seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, 100], 0, 1)\n",
        "    #noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_3d = generator(noise, training=True)\n",
        "      #print(generated_3d.shape)\n",
        "\n",
        "      #print(images.shape)\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_3d, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    #clear_output(wait=True)\n",
        "    #generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-583abf9e2793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time for epoch {} is {} sec'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-583abf9e2793>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Produce images for the GIF as you go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-9-583abf9e2793>:61 train_step  *\n        generated_3d = generator(noise, training=True)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1013 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:255 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected axis -1 of input shape to have value 200 but received input with shape (10, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uGyg6RmcW0q"
      },
      "source": [
        "def jpp(vec, vals):\n",
        "    ss = np.searchsorted(vals, vec)\n",
        "    a = vals[ss - 1]\n",
        "    b = vals[np.minimum(len(vals) - 1, ss)]\n",
        "    return np.where(np.fabs(vec - a) < np.fabs(vec - b), a, b)\n",
        "\n",
        "world = SchematicTools.simplify2(SchematicTools.loadArea(folder + 'mcedit settlement generator/0.schematic'))\n",
        "blocks_type = np.unique(world.flatten())\n",
        "\n",
        "prova = generator(seed, training=False)\n",
        "prova = prova[0, :, :, :, 0].numpy().astype(int)\n",
        "prova = jpp(prova.flatten(), blocks_type)\n",
        "prova.reshape(20, 80, 80)\n",
        "#exportSchematic = SchematicFile(shape=prova.shape)\n",
        "#exportSchematic.blocks = prova\n",
        "#exportSchematic.save(folder + \"mcedit settlement generator/first trial clean.schematic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86BByLcXyeL5",
        "outputId": "bf5ba817-d95a-446d-a3ae-e91c5f84860b"
      },
      "source": [
        "np.unique(prova.flatten())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKdwIF5-cjA6"
      },
      "source": [
        "#completely differente file that manages the generation of structures\n",
        "def generateSettlements(num_examples_to_generate, noise_dim):\n",
        "  #'training' is set to False.\n",
        "  # this is so all layers run in inference mode (batchnorm).\n",
        "  settlements = generator(tf.random.normal([num_examples_to_generate, noise_dim]), training=False)\n",
        "  \n",
        "  for i in range(num_examples_to_generate):\n",
        "    settlement = settlements[i, :, :, :, 0] * 0.5 + 0.5\n",
        "    exportArea = settlement.numpy()#dtype=np.uint8) #uint8??\n",
        "    exportSchematic = SchematicFile(shape=exportArea.shape)\n",
        "    exportSchematic.blocks = exportArea\n",
        "    exportSchematic.save(\"mcedit settlement generator/\" + str(i) + \".schematic\")\n",
        "\n",
        "\"\"\"\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)).expect_partial()\"\"\"\n",
        "\n",
        "generateSettlements(5, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZefdO_imu4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af535bb-ab3a-4284-d665-ce61fcb9a56a"
      },
      "source": [
        "prova = SchematicTools.loadArea(\"mcedit settlement generator/4.schematic\")\n",
        "print(prova)\n",
        "print(np.unique(prova.flatten()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]]\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}