{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minecraft1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "interpreter": {
      "hash": "b64057e63add2b45b1ffc7eab9b09c8889b419c878e2fdf0d08f837f0fc857a7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikros98/Minecraft-settlement-GAN/blob/main/minecraft_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7LwZN1CZ-yr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a08893b-f79e-46fc-c7e6-a1b43090bf55"
      },
      "source": [
        "# code for GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/FProject')\n",
        "folder = '/content/gdrive/MyDrive/FProject/'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn7NxPcARcPx",
        "outputId": "75b5472d-d043-4a0d-feb4-4b907014c932"
      },
      "source": [
        "# auto reload\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "!pip install nbtlib\n",
        "!git clone https://ikros98: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@github.com/ikros98/Minecraft-settlement-GAN.git\n",
        "%cd Minecraft-settlement-GAN/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nbtlib\n",
            "  Downloading nbtlib-1.7.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16 in /usr/local/lib/python3.7/dist-packages (from nbtlib) (1.19.5)\n",
            "Installing collected packages: nbtlib\n",
            "Successfully installed nbtlib-1.7.0\n",
            "Cloning into 'Minecraft-settlement-GAN'...\n",
            "remote: Enumerating objects: 5821, done.\u001b[K\n",
            "remote: Counting objects: 100% (5821/5821), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5721/5721), done.\u001b[K\n",
            "remote: Total 5821 (delta 257), reused 5651 (delta 98), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (5821/5821), 21.32 MiB | 21.99 MiB/s, done.\n",
            "Resolving deltas: 100% (257/257), done.\n",
            "/content/Minecraft-settlement-GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKFD6ceJGW2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13bc281-8032-4642-f3ac-d57dfdc085d5"
      },
      "source": [
        "%cd Minecraft-settlement-GAN/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Minecraft-settlement-GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPrDfxso_EdK",
        "outputId": "c8c2f9bc-4680-4672-ce16-887382f19022"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything up-to-date\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLbL-0kQwJO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9b96a1-0bf1-41b9-a79a-e8e429302144"
      },
      "source": [
        "# preperaing for input 0 1, stone\n",
        "import SchematicTools\n",
        "from schematic import SchematicFile\n",
        "import numpy as np\n",
        "import SchematicTools\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# colab\n",
        "folder = '/content/Minecraft-settlement-GAN/'\n",
        "# local\n",
        "#folder = '/Users/ikros/Documents/GitHub/Minecraft-settlement-GAN/'\n",
        "\n",
        "# function to load the entire dataset\n",
        "def load_dataset():\n",
        "    l = []\n",
        "    for file in os.listdir(folder + \"dataset808016/\"):\n",
        "        settlement = SchematicTools.simplify(SchematicTools.loadArea(folder + \"dataset808016/\" + file))\n",
        "        l.append(settlement)\n",
        "    x = np.array(l)\n",
        "    return (x)\n",
        "\n",
        "X = load_dataset()\n",
        "X = 1 * X\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3], 1).astype(np.float32) #?? to fix float 32, maybe int\n",
        "print(np.count_nonzero(X[0] == 0))\n",
        "print(np.count_nonzero(X[0] == 1))\n",
        "X = np.where(X == 0, -1, X) # Input reshape for tanh. It should work\n",
        "print(\"ciao\")\n",
        "print(np.count_nonzero(X[0] == -1))\n",
        "print(np.count_nonzero(X[0] == 1))\n",
        "\n",
        "print(np.unique(X[0].flatten()))\n",
        "print(X.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95373\n",
            "7027\n",
            "ciao\n",
            "95373\n",
            "7027\n",
            "[-1.  1.]\n",
            "(2500, 16, 80, 80, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWKemDvWiXKA",
        "outputId": "1678f7e3-3d07-4a7c-d8fe-d4342da82517"
      },
      "source": [
        "# preperaing for input real input made of different materials\n",
        "import SchematicTools\n",
        "from schematic import SchematicFile\n",
        "import numpy as np\n",
        "import SchematicTools\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# colab\n",
        "folder = '/content/Minecraft-settlement-GAN/'\n",
        "# local\n",
        "#folder = '/Users/ikros/Documents/GitHub/Minecraft-settlement-GAN/'\n",
        "\n",
        "FILTER1 = np.array([0,17,5,20,50,85,4,67,64,43,61,139,109,98,8,43], dtype=int)\n",
        "enc_FILTER1 = np.array(range(FILTER1.shape[0]))\n",
        "\n",
        "# function to load the entire dataset\n",
        "def load_dataset():\n",
        "    l = []\n",
        "    for file in os.listdir(folder + \"dataset808016/\"):\n",
        "        settlement = SchematicTools.simplify2(SchematicTools.loadArea(folder + \"dataset808016/\" + file))\n",
        "        l.append(settlement)\n",
        "    x = np.array(l)\n",
        "    return (x)\n",
        "\n",
        "X = load_dataset()\n",
        "#X = 1 * X\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3], 1).astype(np.float32) #?? to fix float 32, maybe int\n",
        "X = (X - max(enc_FILTER1)/2) / (max(enc_FILTER1)/2)\n",
        "\n",
        "print(np.unique(X[0].flatten()))\n",
        "print(X.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.         -0.46666667 -0.33333334  0.06666667  1.2666667   1.6666666\n",
            "  4.733333    5.6666665   7.133333    7.5333333   7.9333334  10.333333\n",
            " 12.066667   13.533334   17.533333  ]\n",
            "(2500, 16, 80, 80, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBKoSbs23tNQ"
      },
      "source": [
        "My prova"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Bb6y6Fc37T"
      },
      "source": [
        "# this cleans transforms all the cubes in concrete\n",
        "simpleWorlds = SchematicTools.simplify(SchematicTools.loadArea(folder + 'mcedit settlement generator/0.schematic'))\n",
        "#exporting after simpleWorlds became\n",
        "exportSchematic = SchematicFile(shape=simpleWorlds.shape)\n",
        "exportSchematic.blocks = simpleWorlds\n",
        "exportSchematic.save(folder + \"mcedit settlement generator/sampledExample.schematic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83-hsgU6nGL"
      },
      "source": [
        "My prova 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWzVepS46pY8"
      },
      "source": [
        "# it cleans the file from unnecessary blocks\n",
        "simpleWorlds = SchematicTools.simplify2(SchematicTools.loadArea(folder + 'mcedit settlement generator/0.schematic'))\n",
        "#exporting after simpleWorlds became\n",
        "exportSchematic = SchematicFile(shape=simpleWorlds.shape)\n",
        "exportSchematic.blocks = simpleWorlds\n",
        "exportSchematic.save(folder + \"mcedit settlement generator/sampledExample.schematic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83M4laLdwJPA"
      },
      "source": [
        "# cleaning all the schematics file from not useful \n",
        "for file in os.listdir(folder + \"dataset808020/\"):\n",
        "    settlement = SchematicTools.simplify2(SchematicTools.loadArea(folder + \"dataset808020/\" + file))\n",
        "    exportSchematic = SchematicFile(shape=settlement.shape)\n",
        "    exportSchematic.blocks = settlement\n",
        "    exportSchematic.save(folder + \"clean dataset/\" + file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI_U3paZwJPA"
      },
      "source": [
        "# creating a csv file with the cleaned settlements\n",
        "l = []\n",
        "for file in os.listdir(folder + \"clean dataset/\"):\n",
        "    settlement = SchematicTools.loadArea(folder + \"clean dataset/\" + file)\n",
        "    l.append(settlement.ravel())\n",
        "dataset = pd.DataFrame(l)\n",
        "dataset.to_csv('out.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kn6wlFP3tlg"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "# have to understand how to manage the 30 that becomes 28\n",
        "\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(5*20*20*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((5, 20, 20, 256)))\n",
        "    assert model.output_shape == (None, 5, 20, 20, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(128, (5, 5, 5), strides=(1, 1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 5, 20, 20, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(64, (5, 5, 5), strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 10, 40, 40, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(1, (5, 5, 5), strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 20, 80, 80, 1)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv3D(64, (5, 5, 5), strides=(2, 2, 2), padding='same',\n",
        "                                     input_shape=[20, 80, 80, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv3D(128, (5, 5, 5), strides=(2, 2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp19QaVekyet"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "import random \n",
        "\n",
        "def make_generator_model():\n",
        "  output_size, half, forth, eighth, sixteenth = 80, 40, 20, 10, 5\n",
        "  output_size_h, half_h, forth_h, eighth_h, sixteenth_h = 16, 8, 4, 2, 1\n",
        "  gf_dim = 256 # Dimension of gen filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  model.add(layers.Dense(sixteenth_h*sixteenth*sixteenth*gf_dim, kernel_initializer=tf.random_normal_initializer(stddev=0.02), input_shape=(200,)))\n",
        "  model.add(layers.Reshape((sixteenth_h, sixteenth, sixteenth, gf_dim)))\n",
        "  assert model.output_shape == (None, 1, 5, 5, 256)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim, [eighth_h, eighth, eighth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 2, 10, 10, 256)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/2, [forth_h, forth, forth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim/2, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 4, 20, 20, 128)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/4, [forth_h, forth, forth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim/4, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 8, 40, 40, 64)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(1, [output_size_h, output_size, output_size], strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "  model.add(layers.Conv3DTranspose(1, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "  assert model.output_shape == (None, 16, 80, 80, 1)\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/8, [output_size_h, output_size, output_size], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  #model.add(layers.Reshape([output_size_h, output_size, output_size, gf_dim]))\n",
        "  #assert model.output_shape == (16, 80, 80, 256)\n",
        "  #model.add(layers.Reshape([output_size_h, output_size, output_size, int(gf_dim/8)]))\n",
        "  #assert model.output_shape == (None, 16, 80, 80, 32)\n",
        "  #model.add(layers.Reshape([32, output_size_h, output_size, output_size]))\n",
        "  #assert model.output_shape == (16, 80, 80, 256)\n",
        "\n",
        "  \"\"\"\n",
        "  if sig: \n",
        "    model.add(layers.Activation('sigmoid'))\n",
        "  else:\n",
        "    model.add(layers.Activation('tanh'))\"\"\"\n",
        "  \n",
        "  return model\n",
        "\n",
        "def make_discriminator_model(output_units = 1):\n",
        "  df_dim = 32 # Dimension of discrim filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv3D(df_dim, (4, 4, 4), strides=(2, 2, 2), input_shape=[16, 80, 80, 1],  activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  #model.add(layers.Dropout(0.25))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*2, (4, 4, 4), strides=(2, 2, 2), padding='same'))\n",
        "  #model.add(layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  #model.add(layers.Dropout(0.25))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*4, (4, 4, 4), strides=(2, 2, 2), padding='same'))\n",
        "  #model.add(layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  #model.add(layers.Dropout(0.25))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*8, (4, 4, 4), strides=(2, 2, 2), padding='same'))\n",
        "  #model.add(layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  #model.add(layers.Dropout(0.25))\n",
        "  \n",
        "  #model.add(layers.Flatten())\n",
        "\n",
        "  model.add(layers.Dense(output_units, kernel_initializer=tf.random_normal_initializer(stddev=0.02), input_dim=32))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5XSkcKF3BQy"
      },
      "source": [
        "#WGAN-GP\n",
        "BUFFER_SIZE = 2500\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "noise_dim = 200\n",
        "gp_weight=10.0\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "def gradient_penalty(batch_size, discriminator, real_images, fake_images):\n",
        "  \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "  This loss is calculated on an interpolated image\n",
        "  and added to the discriminator loss.\n",
        "  \"\"\"\n",
        "  # Get the interpolated image\n",
        "  alpha = tf.random.normal([batch_size, 1, 1, 1, 1], 0.0, 1.0)\n",
        "  diff = fake_images - real_images\n",
        "  interpolated = real_images + alpha * diff\n",
        "\n",
        "  with tf.GradientTape() as gp_tape:\n",
        "      gp_tape.watch(interpolated)\n",
        "      # 1. Get the discriminator output for this interpolated image.\n",
        "      pred = discriminator(interpolated, training=True)\n",
        "\n",
        "  # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "  grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "  # 3. Calculate the norm of the gradients.\n",
        "  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "  gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "  return gp\n",
        "\n",
        "# Define the loss functions for the discriminator,\n",
        "# which should be (fake_loss - real_loss).\n",
        "# We will add the gradient penalty later to this loss function.\n",
        "def discriminator_loss(real_img, fake_img):\n",
        "    real_loss = tf.reduce_mean(real_img)\n",
        "    fake_loss = tf.reduce_mean(fake_img)\n",
        "    return fake_loss - real_loss\n",
        "\n",
        "\n",
        "# Define the loss functions for the generator.\n",
        "def generator_loss(fake_img):\n",
        "    return -tf.reduce_mean(fake_img)\n",
        "\n",
        "# try with a different optimisers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "checkpoint_dir = '/content/gdrive/MyDrive/FProject/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(real_images):\n",
        "  if isinstance(real_images, tuple):\n",
        "      real_images = real_images[0]\n",
        "\n",
        "  # Get the batch size\n",
        "  batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "  # For each batch, we are going to perform the\n",
        "  # following steps as laid out in the original paper:\n",
        "  # 1. Train the generator and get the generator loss\n",
        "  # 2. Train the discriminator and get the discriminator loss\n",
        "  # 3. Calculate the gradient penalty\n",
        "  # 4. Multiply this gradient penalty with a constant weight factor\n",
        "  # 5. Add the gradient penalty to the discriminator loss\n",
        "  # 6. Return the generator and discriminator losses as a loss dictionary\n",
        "\n",
        "  # Train the discriminator first. The original paper recommends training\n",
        "  # the discriminator for `x` more steps (typically 5) as compared to\n",
        "  # one step of the generator. Here we will train it for 3 extra steps\n",
        "  # as compared to 5 to reduce the training time.\n",
        "  for i in range(3):\n",
        "      # Get the latent vector\n",
        "      random_latent_vectors = tf.random.normal(\n",
        "          shape=(batch_size, noise_dim)\n",
        "      )\n",
        "      with tf.GradientTape() as tape:\n",
        "          # Generate fake images from the latent vector\n",
        "          fake_images = generator(random_latent_vectors, training=True)\n",
        "          # Get the logits for the fake images\n",
        "          fake_logits = discriminator(fake_images, training=True)\n",
        "          # Get the logits for the real images\n",
        "          real_logits = discriminator(real_images, training=True)\n",
        "\n",
        "          # Calculate the discriminator loss using the fake and real image logits\n",
        "          d_cost = discriminator_loss(real_img=real_logits, fake_img=fake_logits)\n",
        "          # Calculate the gradient penalty\n",
        "          gp = gradient_penalty(batch_size, discriminator, real_images, fake_images)\n",
        "          # Add the gradient penalty to the original discriminator loss\n",
        "          d_loss = d_cost + gp * gp_weight\n",
        "\n",
        "      # Get the gradients w.r.t the discriminator loss\n",
        "      d_gradient = tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "      # Update the weights of the discriminator using the discriminator optimizer\n",
        "      discriminator_optimizer.apply_gradients(\n",
        "          zip(d_gradient, discriminator.trainable_variables)\n",
        "      )\n",
        "\n",
        "  # Train the generator\n",
        "  # Get the latent vector\n",
        "  random_latent_vectors = tf.random.normal(shape=(batch_size, noise_dim))\n",
        "  with tf.GradientTape() as tape:\n",
        "      # Generate fake images using the generator\n",
        "      generated_images = generator(random_latent_vectors, training=True)\n",
        "      # Get the discriminator logits for fake images\n",
        "      gen_img_logits = discriminator(generated_images, training=True)\n",
        "      # Calculate the generator loss\n",
        "      g_loss = generator_loss(gen_img_logits)\n",
        "\n",
        "  # Get the gradients w.r.t the generator loss\n",
        "  gen_gradient = tape.gradient(g_loss, generator.trainable_variables)\n",
        "  # Update the weights of the generator using the generator optimizer\n",
        "  generator_optimizer.apply_gradients(\n",
        "      zip(gen_gradient, generator.trainable_variables)\n",
        "  )\n",
        "  ### return {\"d_loss\": d_loss, \"g_loss\": g_loss}    \n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    print()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8kk7Olzj3SG"
      },
      "source": [
        "def rand_translation(x, ratio=0.125):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    image_size = tf.shape(x)[1:4]\n",
        "    shift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
        "    translation_x = tf.random.uniform([batch_size, 1], -shift[0], shift[0] + 1, dtype=tf.int32)\n",
        "    translation_y = tf.random.uniform([batch_size, 1], -shift[1], shift[1] + 1, dtype=tf.int32)\n",
        "    translation_z = tf.random.uniform([batch_size, 1], -shift[2], shift[2] + 1, dtype=tf.int32)\n",
        "    grid_x = tf.clip_by_value(tf.expand_dims(tf.range(image_size[0], dtype=tf.int32), 0) + translation_x + 1, 0, image_size[0] + 1)\n",
        "    grid_y = tf.clip_by_value(tf.expand_dims(tf.range(image_size[1], dtype=tf.int32), 0) + translation_y + 1, 0, image_size[1] + 1)\n",
        "    grid_z = tf.clip_by_value(tf.expand_dims(tf.range(image_size[2], dtype=tf.int32), 0) + translation_z + 1, 0, image_size[2] + 1)\n",
        "    x = tf.gather_nd(tf.pad(x, [[0, 0], [1, 1], [0, 0], [0, 0], [0, 0]]), tf.expand_dims(grid_x, -1), batch_dims=1)\n",
        "    x = tf.transpose(tf.gather_nd(tf.pad(tf.transpose(x, [0, 2, 1, 3, 4]), [[0, 0], [1, 1], [0, 0], [0, 0], [0, 0]]), tf.expand_dims(grid_y, -1), batch_dims=1), [0, 2, 1, 3, 4])\n",
        "    x = tf.transpose(tf.gather_nd(tf.pad(tf.transpose(x, [0, 2, 1, 3, 4]), [[0, 0], [1, 1], [0, 0], [0, 0], [0, 0]]), tf.expand_dims(grid_z, -1), batch_dims=1), [0, 2, 1, 3, 4])\n",
        "    return x\n",
        "\n",
        "\"\"\"\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    image_size = tf.shape(x)[1:4]\n",
        "    cutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
        "    offset_x = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[0] + (1 - cutout_size[0] % 2), dtype=tf.int32) # [tf.shape(x)[0], 1, 1]?\n",
        "    offset_y = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[1] + (1 - cutout_size[1] % 2), dtype=tf.int32) # [tf.shape(x)[0], 1, 1]?\n",
        "    offset_z = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[2] + (1 - cutout_size[2] % 2), dtype=tf.int32) # [tf.shape(x)[0], 1, 1]?\n",
        "    grid_batch, grid_x, grid_y, grid_z = tf.meshgrid(tf.range(batch_size, dtype=tf.int32), tf.range(cutout_size[0], dtype=tf.int32), tf.range(cutout_size[1], dtype=tf.int32), tf.range(cutout_size[2], dtype=tf.int32), indexing='ij')\n",
        "    cutout_grid = tf.stack([grid_batch, grid_x + offset_x - cutout_size[0] // 2, grid_y + offset_y - cutout_size[1] // 2, grid_z + offset_z - cutout_size[2] // 2], axis=-1)\n",
        "    mask_shape = tf.stack([batch_size, image_size[0], image_size[1, image_size[2]]])\n",
        "    cutout_grid = tf.maximum(cutout_grid, 0)\n",
        "    cutout_grid = tf.minimum(cutout_grid, tf.reshape(mask_shape - 1, [1, 1, 1, 1, 3]))\n",
        "    mask = tf.maximum(1 - tf.scatter_nd(cutout_grid, tf.ones([batch_size, cutout_size[0], cutout_size[1], cutout_size[2]], dtype=tf.float32), mask_shape), 0)\n",
        "    x = x * tf.expand_dims(mask, axis=4)\n",
        "    return x\n",
        "\"\"\"\n",
        "AUGMENT_FNS = {\n",
        "'translation': [rand_translation]#,\n",
        "#'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "def DiffAugment(x, policy=''):\n",
        "    if policy:\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "    return x"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcZ05ZZ2erUV"
      },
      "source": [
        "# Data augmentation trial\n",
        "\n",
        "# DCGAN\n",
        "\n",
        "policy = ''\n",
        "BUFFER_SIZE = 2500\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "noise_dim = 200\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  return cross_entropy(real_output, fake_output)\n",
        "    #real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    #fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    #total_loss = real_loss + fake_loss\n",
        "    #return total_loss\n",
        "\n",
        "def generator_loss(label, output):\n",
        "    return cross_entropy(label, output)\n",
        "    #return -tf.reduce_mean(tf.math.log(fake_output))\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0025, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.00005, beta_1=0.5)\n",
        "\n",
        "checkpoint_dir = '/content/gdrive/MyDrive/FProject/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    # noise vector sampled from normal distribution\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    # Train Discriminator with real labels\n",
        "    with tf.GradientTape() as disc_tape1:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        generated_images = DiffAugment(generated_images, policy=policy, channels_first=True)\n",
        "\n",
        "        real_output = discriminator(DiffAugment(images, policy=policy, channels_first=True), training=True)\n",
        "        real_targets = tf.ones_like(real_output)\n",
        "        disc_loss1 = discriminator_loss(DiffAugment(real_targets, policy=policy, channels_first=True), real_output)\n",
        "        \n",
        "    # gradient calculation for discriminator for real labels    \n",
        "    gradients_of_disc1 = disc_tape1.gradient(disc_loss1, discriminator.trainable_variables)\n",
        "    \n",
        "    # parameters optimization for discriminator for real labels   \n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc1,\\\n",
        "    discriminator.trainable_variables))\n",
        "    \n",
        "    # Train Discriminator with fake labels\n",
        "    with tf.GradientTape() as disc_tape2:\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        fake_targets = tf.zeros_like(fake_output)\n",
        "        disc_loss2 = discriminator_loss(DiffAugment(fake_targets, policy=policy, channels_first=True), fake_output)\n",
        "    # gradient calculation for discriminator for fake labels \n",
        "    gradients_of_disc2 = disc_tape2.gradient(disc_loss2, discriminator.trainable_variables)\n",
        "    \n",
        "    \n",
        "    # parameters optimization for discriminator for fake labels        \n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc2,\\\n",
        "    discriminator.trainable_variables))\n",
        "    \n",
        "    # Train Generator with real labels\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        generated_images = DiffAugment(generated_images, policy=policy, channels_first=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        real_targets = tf.ones_like(fake_output)\n",
        "        gen_loss = generator_loss(DiffAugment(real_targets, policy=policy, channels_first=True), fake_output)\n",
        "\n",
        "    # gradient calculation for generator for real labels     \n",
        "    gradients_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    \n",
        "    # parameters optimization for generator for real labels\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_gen,\\\n",
        "    generator.trainable_variables))    \n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    print()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      # manage batch size like in WGAN\n",
        "      #if image_batch.numpy().shape[0] == 4:\n",
        "        #print(\"shape error\")\n",
        "        #continue\n",
        "      structure = image_batch[0].numpy()[:, :, :, 0]\n",
        "      structure = np.where(structure > 0, 1, 0)\n",
        "      print(structure.shape)\n",
        "      exportSchematic = SchematicFile(shape=structure.shape)\n",
        "      exportSchematic.blocks = structure\n",
        "      exportSchematic.save(folder + \"0\" + \".schematic\")\n",
        "\n",
        "      prova = DiffAugment(image_batch, policy='cutout')\n",
        "      structure = prova[0].numpy()[:, :, :, 0]\n",
        "      print(structure.shape)\n",
        "      structure = np.where(structure > 0, 1, 0)\n",
        "      exportSchematic = SchematicFile(shape=structure.shape)\n",
        "      exportSchematic.blocks = structure\n",
        "      exportSchematic.save(folder + \"1\" + \".schematic\")\n",
        "\n",
        "      break\n",
        "      #train_step(image_batch)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdwZXhTG3jYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "678211ec-1b5d-4ded-c42a-2b5c03797af5"
      },
      "source": [
        "# DCGAN\n",
        "\n",
        "BUFFER_SIZE = 2500\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "noise_dim = 200\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  return cross_entropy(real_output, fake_output)\n",
        "    #real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    #fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    #total_loss = real_loss + fake_loss\n",
        "    #return total_loss\n",
        "\n",
        "def generator_loss(label, output):\n",
        "    return cross_entropy(label, output)\n",
        "    #return -tf.reduce_mean(tf.math.log(fake_output))\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0025, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.00005, beta_1=0.5)\n",
        "\n",
        "checkpoint_dir = '/content/gdrive/MyDrive/FProject/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    # noise vector sampled from normal distribution\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    # Train Discriminator with real labels\n",
        "    with tf.GradientTape() as disc_tape1:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        \n",
        "        real_output = discriminator(images, training=True)\n",
        "        real_targets = tf.ones_like(real_output)\n",
        "        disc_loss1 = discriminator_loss(real_targets, real_output)\n",
        "        \n",
        "    # gradient calculation for discriminator for real labels    \n",
        "    gradients_of_disc1 = disc_tape1.gradient(disc_loss1, discriminator.trainable_variables)\n",
        "    \n",
        "    # parameters optimization for discriminator for real labels   \n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc1,\\\n",
        "    discriminator.trainable_variables))\n",
        "    \n",
        "    # Train Discriminator with fake labels\n",
        "    with tf.GradientTape() as disc_tape2:\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        fake_targets = tf.zeros_like(fake_output)\n",
        "        disc_loss2 = discriminator_loss(fake_targets, fake_output)\n",
        "    # gradient calculation for discriminator for fake labels \n",
        "    gradients_of_disc2 = disc_tape2.gradient(disc_loss2, discriminator.trainable_variables)\n",
        "    \n",
        "    \n",
        "    # parameters optimization for discriminator for fake labels        \n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc2,\\\n",
        "    discriminator.trainable_variables))\n",
        "    \n",
        "    # Train Generator with real labels\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        real_targets = tf.ones_like(fake_output)\n",
        "        gen_loss = generator_loss(real_targets, fake_output)\n",
        "\n",
        "    # gradient calculation for generator for real labels     \n",
        "    gradients_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    \n",
        "    # parameters optimization for generator for real labels\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_gen,\\\n",
        "    generator.trainable_variables))    \n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    print()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      # manage batch size like in WGAN\n",
        "      #if image_batch.numpy().shape[0] == 4:\n",
        "        #print(\"shape error\")\n",
        "        #continue\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Time for epoch 1 is 62.50764441490173 sec\n",
            "\n",
            "Time for epoch 2 is 26.03156852722168 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8an_6J9cXQ4"
      },
      "source": [
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "checkpoint_dir = '/content/gdrive/MyDrive/FProject/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Load our pre-trained model\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)).expect_partial()\n",
        "\n",
        "print(generator.summary())\n",
        "print(discriminator.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3hackEJpqOB",
        "outputId": "00fc96ab-2b4c-4704-c2c3-a3d5ddadc18e"
      },
      "source": [
        "# Generation of settlements made of stone\n",
        "structureCount = 5\n",
        "noise_dim = 200\n",
        "\n",
        "predictions = generator(tf.random.normal([structureCount, noise_dim]), training=False)\n",
        "#print(type(predictions[0])) Eager tensor\n",
        "predictions = predictions.numpy()\n",
        "print(np.count_nonzero(predictions[0] < 0))\n",
        "print(np.count_nonzero(predictions[0] > 0))\n",
        "predictions.shape = [structureCount, 16, 80, 80]\n",
        "for i in range(structureCount):\n",
        "  structure = predictions[i, :, :, :]\n",
        "  #structure = np.where(structure >= 0.0015888, 1, 0)\n",
        "  structure = np.where(structure >= 0, 1, 0)\n",
        "  print(\"generated %s\" % str(structure.shape))\n",
        "  print(np.count_nonzero(structure == 0))\n",
        "  print(np.count_nonzero(structure == 1))\n",
        "  exportSchematic = SchematicFile(shape=structure.shape)\n",
        "  exportSchematic.blocks = structure\n",
        "  exportSchematic.save(folder + str(i) + \".schematic\")\n",
        "  print(\"exported to \" + \"data/generatedExample.schematic\")\n",
        "  print(np.unique(structure.flatten()))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92889\n",
            "9511\n",
            "generated (16, 80, 80)\n",
            "92889\n",
            "9511\n",
            "exported to data/generatedExample.schematic\n",
            "[0 1]\n",
            "generated (16, 80, 80)\n",
            "96153\n",
            "6247\n",
            "exported to data/generatedExample.schematic\n",
            "[0 1]\n",
            "generated (16, 80, 80)\n",
            "95641\n",
            "6759\n",
            "exported to data/generatedExample.schematic\n",
            "[0 1]\n",
            "generated (16, 80, 80)\n",
            "92799\n",
            "9601\n",
            "exported to data/generatedExample.schematic\n",
            "[0 1]\n",
            "generated (16, 80, 80)\n",
            "96080\n",
            "6320\n",
            "exported to data/generatedExample.schematic\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HAv8iF4Sc7c",
        "outputId": "fdce0f6a-451c-4a78-de59-f7f4f949e16c"
      },
      "source": [
        "# Generation of settlements made of different materials\n",
        "def nearest_value(X, values):\n",
        "  for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "      for k in range(X.shape[2]):\n",
        "        X[i][j][k] = values[(np.abs(values-X[i][j][k])).argmin()]\n",
        "  return X\n",
        "\n",
        "def nearest_enc_value(X, values, filter):\n",
        "  for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "      for k in range(X.shape[2]):\n",
        "        X[i][j][k] = filter[values[(np.abs(values-X[i][j][k])).argmin()]]\n",
        "  return X\n",
        "\n",
        "\n",
        "# Generation of settlements made of stone\n",
        "# Filter of materials that I want to keep\n",
        "FILTER1 = np.array([0,17,5,20,50,85,4,67,64,43,61,139,109,98,8,43], dtype=int)\n",
        "enc_FILTER1 = np.array(range(FILTER1.shape[0]))\n",
        "\n",
        "structureCount = 5\n",
        "noise_dim = 200\n",
        "\n",
        "predictions = generator(tf.random.normal([structureCount, noise_dim]), training=False)\n",
        "predictions = predictions * max(enc_FILTER1)/2 + max(enc_FILTER1)/2\n",
        "predictions = predictions.numpy()\n",
        "predictions.shape = [structureCount, 16, 80, 80]\n",
        "for i in range(structureCount):\n",
        "  structure = predictions[i, :, :, :]\n",
        "  structure = nearest_enc_value(structure, enc_FILTER1, FILTER1)\n",
        "  print(\"generated %s\" % str(structure.shape))\n",
        "  exportSchematic = SchematicFile(shape=structure.shape)\n",
        "  exportSchematic.blocks = structure\n",
        "  exportSchematic.save(folder + str(i) + \".schematic\")\n",
        "  print(\"exported to \" + \"data/generatedExample.schematic\")\n",
        "  print(np.unique(structure.flatten()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated (16, 80, 80)\n",
            "exported to data/generatedExample.schematic\n",
            "[  0.   4.   5.   8.  17.  20.  43.  50.  61.  64.  67.  85.  98. 109.\n",
            " 139.]\n",
            "generated (16, 80, 80)\n",
            "exported to data/generatedExample.schematic\n",
            "[  0.   4.   5.   8.  17.  20.  43.  50.  61.  64.  67.  85.  98. 109.\n",
            " 139.]\n",
            "generated (16, 80, 80)\n",
            "exported to data/generatedExample.schematic\n",
            "[  0.   4.   5.   8.  17.  20.  43.  50.  61.  64.  67.  85.  98. 109.\n",
            " 139.]\n",
            "generated (16, 80, 80)\n",
            "exported to data/generatedExample.schematic\n",
            "[  0.   4.   5.   8.  17.  20.  43.  50.  61.  64.  67.  85.  98. 109.\n",
            " 139.]\n",
            "generated (16, 80, 80)\n",
            "exported to data/generatedExample.schematic\n",
            "[  0.   4.   5.   8.  17.  20.  43.  50.  61.  64.  67.  85.  98. 109.\n",
            " 139.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTYpaXjRcH-K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}