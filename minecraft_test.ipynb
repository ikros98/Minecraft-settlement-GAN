{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minecraft1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "interpreter": {
      "hash": "b64057e63add2b45b1ffc7eab9b09c8889b419c878e2fdf0d08f837f0fc857a7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikros98/Minecraft-settlement-GAN/blob/main/minecraft_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7LwZN1CZ-yr"
      },
      "source": [
        "# code for GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/FProject')\n",
        "folder = '/content/gdrive/MyDrive/FProject/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARKoPNG5wJO8"
      },
      "source": [
        "# auto reload\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn7NxPcARcPx",
        "outputId": "83ba5451-6a03-4f81-dc4e-e5f9afc5ef77"
      },
      "source": [
        "!pip install nbtlib\n",
        "!git clone https://ikros98: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@github.com/ikros98/Minecraft-settlement-GAN.git\n",
        "%cd Minecraft-settlement-GAN/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nbtlib\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/0e/b1299899ff764ff40344db572fffaf70b1bc64b44ac8cc7084dd5a5ef0d1/nbtlib-1.7.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy<2.0,>=1.16 in /usr/local/lib/python3.7/dist-packages (from nbtlib) (1.19.5)\n",
            "Installing collected packages: nbtlib\n",
            "Successfully installed nbtlib-1.7.0\n",
            "Cloning into 'Minecraft-settlement-GAN'...\n",
            "remote: Enumerating objects: 3798, done.\u001b[K\n",
            "remote: Counting objects: 100% (3798/3798), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3701/3701), done.\u001b[K\n",
            "remote: Total 3798 (delta 242), reused 3645 (delta 95), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3798/3798), 14.33 MiB | 27.28 MiB/s, done.\n",
            "Resolving deltas: 100% (242/242), done.\n",
            "/content/Minecraft-settlement-GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKFD6ceJGW2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13bc281-8032-4642-f3ac-d57dfdc085d5"
      },
      "source": [
        "%cd Minecraft-settlement-GAN/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Minecraft-settlement-GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPrDfxso_EdK",
        "outputId": "c8c2f9bc-4680-4672-ce16-887382f19022"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything up-to-date\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EefohfYwJO-"
      },
      "source": [
        "# colab\n",
        "folder = '/content/Minecraft-settlement-GAN/'\n",
        "# local\n",
        "#folder = '/Users/ikros/Documents/GitHub/Minecraft-settlement-GAN/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLbL-0kQwJO_"
      },
      "source": [
        "import SchematicTools\n",
        "from schematic import SchematicFile\n",
        "import numpy as np\n",
        "import SchematicTools\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBKoSbs23tNQ"
      },
      "source": [
        "My prova"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Bb6y6Fc37T"
      },
      "source": [
        "# this cleans transforms all the cubes in concrete\n",
        "simpleWorlds = SchematicTools.simplify(SchematicTools.loadArea(folder + 'mcedit settlement generator/0.schematic'))\n",
        "#exporting after simpleWorlds became\n",
        "exportSchematic = SchematicFile(shape=simpleWorlds.shape)\n",
        "exportSchematic.blocks = simpleWorlds\n",
        "exportSchematic.save(folder + \"mcedit settlement generator/sampledExample.schematic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83-hsgU6nGL"
      },
      "source": [
        "My prova 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWzVepS46pY8"
      },
      "source": [
        "# it cleans the file from unnecessary blocks\n",
        "simpleWorlds = SchematicTools.simplify2(SchematicTools.loadArea(folder + 'mcedit settlement generator/0.schematic'))\n",
        "#exporting after simpleWorlds became\n",
        "exportSchematic = SchematicFile(shape=simpleWorlds.shape)\n",
        "exportSchematic.blocks = simpleWorlds\n",
        "exportSchematic.save(folder + \"mcedit settlement generator/sampledExample.schematic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83M4laLdwJPA"
      },
      "source": [
        "# cleaning all the schematics file from not useful \n",
        "for file in os.listdir(folder + \"dataset808020/\"):\n",
        "    settlement = SchematicTools.simplify2(SchematicTools.loadArea(folder + \"dataset808020/\" + file))\n",
        "    exportSchematic = SchematicFile(shape=settlement.shape)\n",
        "    exportSchematic.blocks = settlement\n",
        "    exportSchematic.save(folder + \"clean dataset/\" + file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI_U3paZwJPA"
      },
      "source": [
        "# creating a csv file with the cleaned settlements\n",
        "l = []\n",
        "for file in os.listdir(folder + \"clean dataset/\"):\n",
        "    settlement = SchematicTools.loadArea(folder + \"clean dataset/\" + file)\n",
        "    l.append(settlement.ravel())\n",
        "dataset = pd.DataFrame(l)\n",
        "dataset.to_csv('out.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0kBe5wxwJPB"
      },
      "source": [
        "# function to load the entire dataset\n",
        "def load_dataset():\n",
        "    l = []\n",
        "    for file in os.listdir(folder + \"dataset808016/\"):\n",
        "        settlement = SchematicTools.simplify(SchematicTools.loadArea(folder + \"dataset808016/\" + file))\n",
        "        l.append(settlement)\n",
        "    x = np.array(l)\n",
        "    return (x)\n",
        "\n",
        "X = load_dataset()\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3], 1) #?? to fix float 32, maybe int"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2yf8BsXHbDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c549e2c3-6b74-4fd7-c8e5-df7109154e0a"
      },
      "source": [
        "print(np.unique(X[0].flatten()))\n",
        "print(X.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False  True]\n",
            "(500, 16, 80, 80, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-D6EnwgGfmO"
      },
      "source": [
        "# normalizing the images to [-1, 1]\n",
        "def normalise(settlement):\n",
        "  settlement = tf.cast(settlement, tf.float32)\n",
        "  settlement = (settlement / 69.5) - 1\n",
        "  return settlement\n",
        "\n",
        "prova = normalise(X[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzDn14IEH_CW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c0b2067-a3b4-4aac-8a97-758ecb12da14"
      },
      "source": [
        "np.unique(prova.numpy().flatten())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kn6wlFP3tlg"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "# have to understand how to manage the 30 that becomes 28\n",
        "\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(5*20*20*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((5, 20, 20, 256)))\n",
        "    assert model.output_shape == (None, 5, 20, 20, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(128, (5, 5, 5), strides=(1, 1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 5, 20, 20, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(64, (5, 5, 5), strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 10, 40, 40, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv3DTranspose(1, (5, 5, 5), strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 20, 80, 80, 1)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv3D(64, (5, 5, 5), strides=(2, 2, 2), padding='same',\n",
        "                                     input_shape=[20, 80, 80, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv3D(128, (5, 5, 5), strides=(2, 2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5XSkcKF3BQy"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "generated_image = generator(noise, training=False)\n",
        "\"\"\"\n",
        "#plt.imshow(generated_image[0, :, :, :, 0], cmap='gray')\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "def make_ax(grid=False):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca(projection='3d')\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"y\")\n",
        "    ax.set_zlabel(\"z\")#\n",
        "    ax.grid(grid)\n",
        "    return ax\n",
        "\n",
        "ax = make_ax(True)\n",
        "ax.voxels(generated_image[0, :, :, :, 0], edgecolors='gray', shade=False)\n",
        "plt.show()\"\"\"\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "EPOCHS = 400\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    #clear_output(wait=True)\n",
        "    #generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "\n",
        "  preview = np.empty((0, 80 * example_grid,3), np.uint8)\n",
        "  for i in range(example_grid):\n",
        "      row = np.empty((80, 0, 3), np.uint8)\n",
        "      for j in range(example_grid):\n",
        "          imageR = predictions[i*example_grid + j, :, :, :, 0] * 127.5 + 127.5\n",
        "          imageG = np.average(imageR, axis=0)\n",
        "          image = np.stack((imageR[0], imageG, imageG), axis=2)\n",
        "          row = np.hstack((row, image))\n",
        "      preview = np.vstack((preview, row))\n",
        "\n",
        "  outputimage = PIL.Image.fromarray(preview.astype(np.uint8))\n",
        "  outputimage.save('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  return outputimage\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp19QaVekyet"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "import random \n",
        "\n",
        "def make_generator_model(sig = False):\n",
        "  output_size, half, forth, eighth, sixteenth = 80, 40, 20, 10, 5\n",
        "  output_size_h, half_h, forth_h, eighth_h, sixteenth_h = 16, 8, 4, 2, 1\n",
        "  gf_dim = 256 # Dimension of gen filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  model.add(layers.Dense(sixteenth_h*sixteenth*sixteenth*gf_dim, kernel_initializer=tf.random_normal_initializer(stddev=0.02), input_shape=(100,)))\n",
        "  model.add(layers.Reshape((sixteenth_h, sixteenth, sixteenth, gf_dim)))\n",
        "  assert model.output_shape == (None, 1, 5, 5, 256)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim, [eighth_h, eighth, eighth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 2, 10, 10, 256)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/2, [forth_h, forth, forth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim/2, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 4, 20, 20, 128)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/4, [forth_h, forth, forth], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  model.add(layers.Conv3DTranspose(gf_dim/4, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 8, 40, 40, 64)\n",
        "  model.add(layers.BatchNormalization(gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  #model.add(layers.Conv3DTranspose(1, [output_size_h, output_size, output_size], strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "  model.add(layers.Conv3DTranspose(1, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "  assert model.output_shape == (None, 16, 80, 80, 1)\n",
        "  #model.add(layers.Conv3DTranspose(gf_dim/8, [output_size_h, output_size, output_size], strides=(2, 2, 2), padding='same', use_bias=False))\n",
        "  #model.add(layers.Reshape([output_size_h, output_size, output_size, gf_dim]))\n",
        "  #assert model.output_shape == (16, 80, 80, 256)\n",
        "  #model.add(layers.Reshape([output_size_h, output_size, output_size, int(gf_dim/8)]))\n",
        "  #assert model.output_shape == (None, 16, 80, 80, 32)\n",
        "  #model.add(layers.Reshape([32, output_size_h, output_size, output_size]))\n",
        "  #assert model.output_shape == (16, 80, 80, 256)\n",
        "\n",
        "  \"\"\"\n",
        "  if sig: \n",
        "    model.add(layers.Activation('sigmoid'))\n",
        "  else:\n",
        "    model.add(layers.Activation('tanh'))\"\"\"\n",
        "  \n",
        "  return model\n",
        "\n",
        "def make_discriminator_model(sig = False, output_units = 1):\n",
        "  df_dim = 32 # Dimension of discrim filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv3D(df_dim, (4, 4, 4), strides=(2, 2, 2), input_shape=[16, 80, 80, 1],  activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*2, (4, 4, 4), strides=(2, 2, 2), activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*4, (4, 4, 4), strides=(2, 2, 2), activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*8, (4, 4, 4), strides=(2, 2, 2), activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
        "  \n",
        "  #model.add(layers.Flatten())\n",
        "\n",
        "  model.add(layers.Dense(output_units, kernel_initializer=tf.random_normal_initializer(stddev=0.02), input_dim=32))\n",
        "\n",
        "  if sig: \n",
        "    model.add(layers.Activation('sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdwZXhTG3jYE",
        "outputId": "6a0de5d4-f1ba-4e7f-bff6-808a52beab40"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "BUFFER_SIZE = 500\n",
        "BATCH_SIZE = 10\n",
        "EPOCHS = 200\n",
        "noise_dim = 100\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "#cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    return -tf.reduce_mean(tf.math.log(real_output) + tf.math.log(1. - fake_output))\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return -tf.reduce_mean(tf.math.log(fake_output))\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0025, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.00005, beta_1=0.5)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim], 0, 1)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_3d = generator(noise, training=True)\n",
        "      #print(generated_3d.shape)\n",
        "\n",
        "      #print(images.shape)\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_3d, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    #print(\"cau\")\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "      #print(\"bau\")\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    #clear_output(wait=True)\n",
        "    #generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 7.914228200912476 sec\n",
            "Time for epoch 2 is 6.487391233444214 sec\n",
            "Time for epoch 3 is 6.486432075500488 sec\n",
            "Time for epoch 4 is 6.488195896148682 sec\n",
            "Time for epoch 5 is 6.487205266952515 sec\n",
            "Time for epoch 6 is 6.483150482177734 sec\n",
            "Time for epoch 7 is 6.486040830612183 sec\n",
            "Time for epoch 8 is 6.487069368362427 sec\n",
            "Time for epoch 9 is 6.485615253448486 sec\n",
            "Time for epoch 10 is 6.484725713729858 sec\n",
            "Time for epoch 11 is 6.484479665756226 sec\n",
            "Time for epoch 12 is 6.482148170471191 sec\n",
            "Time for epoch 13 is 6.485538482666016 sec\n",
            "Time for epoch 14 is 6.483413219451904 sec\n",
            "Time for epoch 15 is 6.482975006103516 sec\n",
            "Time for epoch 16 is 6.492110729217529 sec\n",
            "Time for epoch 17 is 6.507468938827515 sec\n",
            "Time for epoch 18 is 6.499154090881348 sec\n",
            "Time for epoch 19 is 6.504778623580933 sec\n",
            "Time for epoch 20 is 6.92838716506958 sec\n",
            "Time for epoch 21 is 6.450855493545532 sec\n",
            "Time for epoch 22 is 6.525705814361572 sec\n",
            "Time for epoch 23 is 6.531426906585693 sec\n",
            "Time for epoch 24 is 6.5206193923950195 sec\n",
            "Time for epoch 25 is 6.5129547119140625 sec\n",
            "Time for epoch 26 is 6.499472379684448 sec\n",
            "Time for epoch 27 is 6.510181427001953 sec\n",
            "Time for epoch 28 is 6.503478288650513 sec\n",
            "Time for epoch 29 is 6.521300554275513 sec\n",
            "Time for epoch 30 is 6.496227979660034 sec\n",
            "Time for epoch 31 is 6.515692710876465 sec\n",
            "Time for epoch 32 is 6.515785217285156 sec\n",
            "Time for epoch 33 is 6.51842474937439 sec\n",
            "Time for epoch 34 is 6.528383255004883 sec\n",
            "Time for epoch 35 is 6.51346755027771 sec\n",
            "Time for epoch 36 is 6.5169079303741455 sec\n",
            "Time for epoch 37 is 6.520632743835449 sec\n",
            "Time for epoch 38 is 6.5229058265686035 sec\n",
            "Time for epoch 39 is 6.539517402648926 sec\n",
            "Time for epoch 40 is 6.865028619766235 sec\n",
            "Time for epoch 41 is 6.411655902862549 sec\n",
            "Time for epoch 42 is 6.515765905380249 sec\n",
            "Time for epoch 43 is 6.520614385604858 sec\n",
            "Time for epoch 44 is 6.531675815582275 sec\n",
            "Time for epoch 45 is 6.516116142272949 sec\n",
            "Time for epoch 46 is 6.624226093292236 sec\n",
            "Time for epoch 47 is 6.4266791343688965 sec\n",
            "Time for epoch 48 is 6.510239601135254 sec\n",
            "Time for epoch 49 is 6.532581329345703 sec\n",
            "Time for epoch 50 is 6.528393983840942 sec\n",
            "Time for epoch 51 is 6.516993284225464 sec\n",
            "Time for epoch 52 is 6.52279257774353 sec\n",
            "Time for epoch 53 is 6.514680862426758 sec\n",
            "Time for epoch 54 is 6.5195229053497314 sec\n",
            "Time for epoch 55 is 6.507164478302002 sec\n",
            "Time for epoch 56 is 6.521968126296997 sec\n",
            "Time for epoch 57 is 6.4994306564331055 sec\n",
            "Time for epoch 58 is 6.507699489593506 sec\n",
            "Time for epoch 59 is 6.517455339431763 sec\n",
            "Time for epoch 60 is 6.850519418716431 sec\n",
            "Time for epoch 61 is 6.420223951339722 sec\n",
            "Time for epoch 62 is 6.517492294311523 sec\n",
            "Time for epoch 63 is 6.520220994949341 sec\n",
            "Time for epoch 64 is 6.522173166275024 sec\n",
            "Time for epoch 65 is 6.519126653671265 sec\n",
            "Time for epoch 66 is 6.527192115783691 sec\n",
            "Time for epoch 67 is 6.541626453399658 sec\n",
            "Time for epoch 68 is 6.511489152908325 sec\n",
            "Time for epoch 69 is 6.52758526802063 sec\n",
            "Time for epoch 70 is 6.508280277252197 sec\n",
            "Time for epoch 71 is 6.5159876346588135 sec\n",
            "Time for epoch 72 is 6.524215221405029 sec\n",
            "Time for epoch 73 is 6.529983997344971 sec\n",
            "Time for epoch 74 is 6.513562440872192 sec\n",
            "Time for epoch 75 is 6.515590190887451 sec\n",
            "Time for epoch 76 is 6.515830993652344 sec\n",
            "Time for epoch 77 is 6.5218589305877686 sec\n",
            "Time for epoch 78 is 6.545240163803101 sec\n",
            "Time for epoch 79 is 6.539247035980225 sec\n",
            "Time for epoch 80 is 6.847092628479004 sec\n",
            "Time for epoch 81 is 6.415351867675781 sec\n",
            "Time for epoch 82 is 6.508052825927734 sec\n",
            "Time for epoch 83 is 6.5147926807403564 sec\n",
            "Time for epoch 84 is 6.512823581695557 sec\n",
            "Time for epoch 85 is 6.51668906211853 sec\n",
            "Time for epoch 86 is 6.525274276733398 sec\n",
            "Time for epoch 87 is 6.528889894485474 sec\n",
            "Time for epoch 88 is 6.51739501953125 sec\n",
            "Time for epoch 89 is 6.515318155288696 sec\n",
            "Time for epoch 90 is 6.531797885894775 sec\n",
            "Time for epoch 91 is 6.530198097229004 sec\n",
            "Time for epoch 92 is 6.54263162612915 sec\n",
            "Time for epoch 93 is 6.526662588119507 sec\n",
            "Time for epoch 94 is 6.538045167922974 sec\n",
            "Time for epoch 95 is 6.619739770889282 sec\n",
            "Time for epoch 96 is 6.421911001205444 sec\n",
            "Time for epoch 97 is 6.519534111022949 sec\n",
            "Time for epoch 98 is 6.53480863571167 sec\n",
            "Time for epoch 99 is 6.5271642208099365 sec\n",
            "Time for epoch 100 is 6.862149477005005 sec\n",
            "Time for epoch 101 is 6.465869426727295 sec\n",
            "Time for epoch 102 is 6.518593072891235 sec\n",
            "Time for epoch 103 is 6.520941972732544 sec\n",
            "Time for epoch 104 is 6.522563695907593 sec\n",
            "Time for epoch 105 is 6.520678281784058 sec\n",
            "Time for epoch 106 is 6.535503149032593 sec\n",
            "Time for epoch 107 is 6.518492698669434 sec\n",
            "Time for epoch 108 is 6.525464773178101 sec\n",
            "Time for epoch 109 is 6.515226125717163 sec\n",
            "Time for epoch 110 is 6.517564296722412 sec\n",
            "Time for epoch 111 is 6.512890100479126 sec\n",
            "Time for epoch 112 is 6.508812189102173 sec\n",
            "Time for epoch 113 is 6.5115203857421875 sec\n",
            "Time for epoch 114 is 6.5271360874176025 sec\n",
            "Time for epoch 115 is 6.636686086654663 sec\n",
            "Time for epoch 116 is 6.4316017627716064 sec\n",
            "Time for epoch 117 is 6.623097658157349 sec\n",
            "Time for epoch 118 is 6.433738708496094 sec\n",
            "Time for epoch 119 is 6.517308712005615 sec\n",
            "Time for epoch 120 is 6.865753650665283 sec\n",
            "Time for epoch 121 is 6.434794902801514 sec\n",
            "Time for epoch 122 is 6.524936676025391 sec\n",
            "Time for epoch 123 is 6.517256498336792 sec\n",
            "Time for epoch 124 is 6.531440019607544 sec\n",
            "Time for epoch 125 is 6.536269664764404 sec\n",
            "Time for epoch 126 is 6.513854265213013 sec\n",
            "Time for epoch 127 is 6.526271104812622 sec\n",
            "Time for epoch 128 is 6.510721445083618 sec\n",
            "Time for epoch 129 is 6.506883859634399 sec\n",
            "Time for epoch 130 is 6.508541822433472 sec\n",
            "Time for epoch 131 is 6.525853872299194 sec\n",
            "Time for epoch 132 is 6.528346061706543 sec\n",
            "Time for epoch 133 is 6.509607791900635 sec\n",
            "Time for epoch 134 is 6.528925657272339 sec\n",
            "Time for epoch 135 is 6.5372350215911865 sec\n",
            "Time for epoch 136 is 6.50807785987854 sec\n",
            "Time for epoch 137 is 6.515901327133179 sec\n",
            "Time for epoch 138 is 6.5276970863342285 sec\n",
            "Time for epoch 139 is 6.5428102016448975 sec\n",
            "Time for epoch 140 is 6.851919412612915 sec\n",
            "Time for epoch 141 is 6.44454026222229 sec\n",
            "Time for epoch 142 is 6.524676322937012 sec\n",
            "Time for epoch 143 is 6.518030166625977 sec\n",
            "Time for epoch 144 is 6.50035834312439 sec\n",
            "Time for epoch 145 is 6.5242204666137695 sec\n",
            "Time for epoch 146 is 6.528692960739136 sec\n",
            "Time for epoch 147 is 6.537062168121338 sec\n",
            "Time for epoch 148 is 6.513577938079834 sec\n",
            "Time for epoch 149 is 6.5238800048828125 sec\n",
            "Time for epoch 150 is 6.537389516830444 sec\n",
            "Time for epoch 151 is 6.5168375968933105 sec\n",
            "Time for epoch 152 is 6.515799283981323 sec\n",
            "Time for epoch 153 is 6.52891993522644 sec\n",
            "Time for epoch 154 is 6.51783299446106 sec\n",
            "Time for epoch 155 is 6.5232765674591064 sec\n",
            "Time for epoch 156 is 6.538166522979736 sec\n",
            "Time for epoch 157 is 6.53426194190979 sec\n",
            "Time for epoch 158 is 6.525606393814087 sec\n",
            "Time for epoch 159 is 6.527244567871094 sec\n",
            "Time for epoch 160 is 6.863022089004517 sec\n",
            "Time for epoch 161 is 6.454568862915039 sec\n",
            "Time for epoch 162 is 6.525082349777222 sec\n",
            "Time for epoch 163 is 6.518101692199707 sec\n",
            "Time for epoch 164 is 6.514682054519653 sec\n",
            "Time for epoch 165 is 6.524589538574219 sec\n",
            "Time for epoch 166 is 6.533543109893799 sec\n",
            "Time for epoch 167 is 6.51873254776001 sec\n",
            "Time for epoch 168 is 6.537120819091797 sec\n",
            "Time for epoch 169 is 6.519739627838135 sec\n",
            "Time for epoch 170 is 6.5347630977630615 sec\n",
            "Time for epoch 171 is 6.51958966255188 sec\n",
            "Time for epoch 172 is 6.530489921569824 sec\n",
            "Time for epoch 173 is 6.533737897872925 sec\n",
            "Time for epoch 174 is 6.530914306640625 sec\n",
            "Time for epoch 175 is 6.6470842361450195 sec\n",
            "Time for epoch 176 is 6.431576728820801 sec\n",
            "Time for epoch 177 is 6.524820804595947 sec\n",
            "Time for epoch 178 is 6.545180797576904 sec\n",
            "Time for epoch 179 is 6.528886318206787 sec\n",
            "Time for epoch 180 is 6.858539581298828 sec\n",
            "Time for epoch 181 is 10.292293548583984 sec\n",
            "Time for epoch 182 is 6.473214626312256 sec\n",
            "Time for epoch 183 is 6.522097826004028 sec\n",
            "Time for epoch 184 is 6.5140721797943115 sec\n",
            "Time for epoch 185 is 6.522708177566528 sec\n",
            "Time for epoch 186 is 6.512134075164795 sec\n",
            "Time for epoch 187 is 6.549444198608398 sec\n",
            "Time for epoch 188 is 6.520695209503174 sec\n",
            "Time for epoch 189 is 6.512933731079102 sec\n",
            "Time for epoch 190 is 6.524266958236694 sec\n",
            "Time for epoch 191 is 6.517934083938599 sec\n",
            "Time for epoch 192 is 6.515038728713989 sec\n",
            "Time for epoch 193 is 6.5225019454956055 sec\n",
            "Time for epoch 194 is 6.507561206817627 sec\n",
            "Time for epoch 195 is 6.509967803955078 sec\n",
            "Time for epoch 196 is 6.531666994094849 sec\n",
            "Time for epoch 197 is 6.5223472118377686 sec\n",
            "Time for epoch 198 is 6.514084339141846 sec\n",
            "Time for epoch 199 is 6.522162675857544 sec\n",
            "Time for epoch 200 is 6.868498802185059 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "3-d0sEHK4YUN",
        "outputId": "73a5c06b-05b0-4b32-e796-0568f741d16c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "BUFFER_SIZE = 500\n",
        "BATCH_SIZE = 10\n",
        "EPOCHS = 200\n",
        "noise_dim = 100\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([BATCH_SIZE, noise_dim], 0, 1)\n",
        "#generated_image = generator(noise, training=False)\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "#decision = discriminator(generated_image)\n",
        "#print(decision)\n",
        "\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0025, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.00005, beta_1=0.5)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "#seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, 100], 0, 1)\n",
        "    #noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_3d = generator(noise, training=True)\n",
        "      #print(generated_3d.shape)\n",
        "\n",
        "      #print(images.shape)\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_3d, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    #print(\"cau\")\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "      #print(\"bau\")\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    #clear_output(wait=True)\n",
        "    #generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "    # Save the model every 5 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 38.80029606819153 sec\n",
            "Time for epoch 2 is 6.495237350463867 sec\n",
            "Time for epoch 3 is 6.495731353759766 sec\n",
            "Time for epoch 4 is 6.49727988243103 sec\n",
            "Time for epoch 5 is 6.495002269744873 sec\n",
            "Time for epoch 6 is 6.493052959442139 sec\n",
            "Time for epoch 7 is 6.497915267944336 sec\n",
            "Time for epoch 8 is 6.489342927932739 sec\n",
            "Time for epoch 9 is 6.49174690246582 sec\n",
            "Time for epoch 10 is 6.494195222854614 sec\n",
            "Time for epoch 11 is 6.492063999176025 sec\n",
            "Time for epoch 12 is 6.493362903594971 sec\n",
            "Time for epoch 13 is 6.492645502090454 sec\n",
            "Time for epoch 14 is 6.491785764694214 sec\n",
            "Time for epoch 15 is 6.495347499847412 sec\n",
            "Time for epoch 16 is 6.494359493255615 sec\n",
            "Time for epoch 17 is 6.491761684417725 sec\n",
            "Time for epoch 18 is 6.492208242416382 sec\n",
            "Time for epoch 19 is 6.4947731494903564 sec\n",
            "Time for epoch 20 is 6.908557653427124 sec\n",
            "Time for epoch 21 is 6.447702646255493 sec\n",
            "Time for epoch 22 is 6.516195058822632 sec\n",
            "Time for epoch 23 is 6.515777826309204 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4b49c06ee13c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time for epoch {} is {} sec'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-4b49c06ee13c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m       \u001b[0;31m#print(\"bau\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uGyg6RmcW0q"
      },
      "source": [
        "def jpp(vec, vals):\n",
        "    ss = np.searchsorted(vals, vec)\n",
        "    a = vals[ss - 1]\n",
        "    b = vals[np.minimum(len(vals) - 1, ss)]\n",
        "    return np.where(np.fabs(vec - a) < np.fabs(vec - b), a, b)\n",
        "\n",
        "world = SchematicTools.simplify2(SchematicTools.loadArea(folder + 'mcedit settlement generator/0.schematic'))\n",
        "blocks_type = np.unique(world.flatten())\n",
        "\n",
        "prova = generator(seed, training=False)\n",
        "prova = prova[0, :, :, :, 0].numpy().astype(int)\n",
        "prova = jpp(prova.flatten(), blocks_type)\n",
        "prova.reshape(20, 80, 80)\n",
        "#exportSchematic = SchematicFile(shape=prova.shape)\n",
        "#exportSchematic.blocks = prova\n",
        "#exportSchematic.save(folder + \"mcedit settlement generator/first trial clean.schematic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86BByLcXyeL5",
        "outputId": "bf5ba817-d95a-446d-a3ae-e91c5f84860b"
      },
      "source": [
        "np.unique(prova.flatten())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKdwIF5-cjA6"
      },
      "source": [
        "#completely differente file that manages the generation of structures\n",
        "def generateSettlements(num_examples_to_generate, noise_dim):\n",
        "  #'training' is set to False.\n",
        "  # this is so all layers run in inference mode (batchnorm).\n",
        "  settlements = generator(tf.random.normal([num_examples_to_generate, noise_dim]), training=False)\n",
        "  \n",
        "  for i in range(num_examples_to_generate):\n",
        "    settlement = settlements[i, :, :, :, 0] * 0.5 + 0.5\n",
        "    exportArea = settlement.numpy()#dtype=np.uint8) #uint8??\n",
        "    exportSchematic = SchematicFile(shape=exportArea.shape)\n",
        "    exportSchematic.blocks = exportArea\n",
        "    exportSchematic.save(\"mcedit settlement generator/\" + str(i) + \".schematic\")\n",
        "\n",
        "\"\"\"\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)).expect_partial()\"\"\"\n",
        "\n",
        "generateSettlements(5, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZefdO_imu4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af535bb-ab3a-4284-d665-ce61fcb9a56a"
      },
      "source": [
        "prova = SchematicTools.loadArea(\"mcedit settlement generator/4.schematic\")\n",
        "print(prova)\n",
        "print(np.unique(prova.flatten()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]]\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}