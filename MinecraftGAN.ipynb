{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "minecraft1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "interpreter": {
      "hash": "b64057e63add2b45b1ffc7eab9b09c8889b419c878e2fdf0d08f837f0fc857a7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Generative Design in Minecraft"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# auto reload\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn7NxPcARcPx",
        "outputId": "6f116a3d-4893-488e-ae6f-9fe8545cca77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing libraries in the machine:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Example\n",
        "!pip install nbtlib"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the data set to load:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0-1 data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# preperaing for input 0 1, stone\n",
        "import SchematicTools\n",
        "from schematic import SchematicFile\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# folder\n",
        "folder = './'\n",
        "\n",
        "# function that loads the entire dataset\n",
        "def load_dataset():\n",
        "    l = []\n",
        "    # iterating in every file of the data set folder\n",
        "    # where every file is a settlement\n",
        "    for file in os.listdir(folder + \"dataset808016/\"):\n",
        "        # translating the settlement from schematic to nbt\n",
        "        settlement = SchematicTools.simplify(SchematicTools.loadArea\n",
        "                                            (folder + \"dataset808016/\" + file))\n",
        "        l.append(settlement)\n",
        "    # converting everything in numpy format\n",
        "    x = np.array(l)\n",
        "    return (x)\n",
        "\n",
        "X = load_dataset()\n",
        "# the data set will be in boolean, multiplying to 1 will translate\n",
        "# True to 1 and False to 0\n",
        "X = 1 * X\n",
        "# adding bias\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], \n",
        "                          X.shape[3], 1).astype(np.float32) \n",
        "# normalising the data for tanh\n",
        "X = np.where(X == 0, -1, X) "
      ],
      "outputs": [],
      "metadata": {
        "id": "TLbL-0kQwJO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14383534-6254-4ac4-d940-c28c37ad8226"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### all materials data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# preperaing for real input made of different materials\n",
        "import SchematicTools\n",
        "from schematic import SchematicFile\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# folder\n",
        "folder = './'\n",
        "\n",
        "# function that loads the entire dataset\n",
        "\n",
        "# possibility to encode the data\n",
        "def encoding_value(X):\n",
        "  for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "      for k in range(X.shape[2]):\n",
        "        X[i][j][k] = np.where(FILTER1==X[i][j][k])[0][0]\n",
        "  return X\n",
        "\n",
        "# array that contains all the IDs of different materials that are\n",
        "# present in the settlements\n",
        "FILTER1 = np.array([0,4,5,8,17,20,43,50,61,64,67,85,98,109,139], dtype=int)\n",
        "# possibility to encode the data\n",
        "#enc_FILTER1 = np.array(range(FILTER1.shape[0]))\n",
        "\n",
        "# function that loads the entire dataset\n",
        "def load_dataset():\n",
        "    l = []\n",
        "    # iterating in every file of the data set folder\n",
        "    # where every file is a settlement\n",
        "    for file in os.listdir(folder + \"dataset808016/\"):\n",
        "        # translating the data in nbt\n",
        "        settlement = SchematicTools.simplify2(SchematicTools.loadArea\n",
        "                                              (folder + \"dataset808016/\" + file))\n",
        "        l.append(settlement)\n",
        "    # translating to numpy format\n",
        "    x = np.array(l)\n",
        "    return (x)\n",
        "\n",
        "X = load_dataset()\n",
        "# adding the bias to the data set\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], \n",
        "                          X.shape[3], 1).astype(np.float32)\n",
        "# normalising the data for tanh\n",
        "X = (X - max(FILTER1)/2) / (max(FILTER1)/2)\n",
        "# possibility to encode the data\n",
        "#for structure in X:\n",
        "#  structure = encoding_value(structure)\n",
        "#print(np.unique(X[0].flatten()))\n",
        "#print(X.shape)\n",
        "#X = (X - max(enc_FILTER1)/2) / (max(enc_FILTER1)/2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "kWKemDvWiXKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da85d9a-f6e0-48ff-af34-4c406837d1cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose between the two versions of the Discriminator and Generator"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 2"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "source": [
        "# Version 2 of the Discriminator and Generator\n",
        "from tensorflow.keras import layers\n",
        "import random \n",
        "\n",
        "def make_generator_model():\n",
        "  output_size, half, forth, eighth, sixteenth = 80, 40, 20, 10, 5\n",
        "  output_size_h, half_h, forth_h, eighth_h, sixteenth_h = 16, 8, 4, 2, 1\n",
        "  filt_dim = 256 # Dimension of gen filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  model.add(layers.Dense(sixteenth_h*sixteenth*sixteenth*filt_dim, \n",
        "            kernel_initializer=tf.random_normal_initializer(stddev=0.02), \n",
        "            input_shape=(200,)))\n",
        "  model.add(layers.Reshape((sixteenth_h, sixteenth, sixteenth, filt_dim)))\n",
        "  assert model.output_shape == (None, 1, 5, 5, 256)\n",
        "  model.add(layers.BatchNormalization(\n",
        "            gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  model.add(layers.Conv3DTranspose(filt_dim, [4, 4, 4], strides=(2, 2, 2), \n",
        "                                   padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 2, 10, 10, 256)\n",
        "  model.add(layers.BatchNormalization(\n",
        "      gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  model.add(layers.Conv3DTranspose(filt_dim/2, [4, 4, 4], strides=(2, 2, 2), \n",
        "                                   padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 4, 20, 20, 128)\n",
        "  model.add(layers.BatchNormalization(\n",
        "      gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  model.add(layers.Conv3DTranspose(filt_dim/4, [4, 4, 4], strides=(2, 2, 2), \n",
        "                                   padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 8, 40, 40, 64)\n",
        "  model.add(layers.BatchNormalization(\n",
        "      gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  model.add(layers.Conv3DTranspose(1, [4, 4, 4], strides=(2, 2, 2), \n",
        "                                   padding='same', use_bias=False, \n",
        "                                   activation='tanh'))\n",
        "  assert model.output_shape == (None, 16, 80, 80, 1)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def make_discriminator_model(output_units = 1):\n",
        "  df_dim = 32 # Dimension of discrim filters in first conv layer\n",
        "  filt_dim = 32\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv3D(df_dim, (4, 4, 4), strides=(2, 2, 2), \n",
        "                          input_shape=[16, 80, 80, 1],  \n",
        "                          activation=tf.keras.layers.LeakyReLU(), \n",
        "                          padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*2, (4, 4, 4), strides=(2, 2, 2), \n",
        "                          padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*4, (4, 4, 4), strides=(2, 2, 2), \n",
        "                          padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*8, (4, 4, 4), strides=(2, 2, 2), \n",
        "                          padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  \n",
        "  model.add(layers.Dense(output_units, \n",
        "            kernel_initializer=tf.random_normal_initializer(stddev=0.02), \n",
        "            input_dim=32))\n",
        "  return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "XHAbl6m9ZFhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 1"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "# Version 1 of Discriminator and Generator\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import random \n",
        "\n",
        "def make_generator_model():\n",
        "  output_size, half, forth, eighth, sixteenth = 80, 40, 20, 10, 5\n",
        "  output_size_h, half_h, forth_h, eighth_h, sixteenth_h = 16, 8, 4, 2, 1\n",
        "  filt_dim = 256 # Dimension of gen filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  model.add(layers.Dense(sixteenth_h*sixteenth*sixteenth*filt_dim, \n",
        "            kernel_initializer=tf.random_normal_initializer(stddev=0.02), \n",
        "            input_shape=(100,)))\n",
        "  model.add(layers.Reshape((sixteenth_h, sixteenth, sixteenth, filt_dim)))\n",
        "  assert model.output_shape == (None, 1, 5, 5, 256)\n",
        "  model.add(layers.BatchNormalization(\n",
        "      gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  model.add(layers.Conv3DTranspose(filt_dim, [4, 4, 4], strides=(2, 2, 2), \n",
        "                                   padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 2, 10, 10, 256)\n",
        "  model.add(layers.BatchNormalization(\n",
        "      gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  model.add(layers.Conv3DTranspose(filt_dim/2, [4, 4, 4], strides=(2, 2, 2), \n",
        "                                   padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 4, 20, 20, 128)\n",
        "  model.add(layers.BatchNormalization(\n",
        "      gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  model.add(layers.Conv3DTranspose(filt_dim/4, [4, 4, 4], strides=(2, 2, 2),\n",
        "                                   padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 8, 40, 40, 64)\n",
        "  model.add(layers.BatchNormalization(\n",
        "      gamma_initializer=tf.random_normal_initializer(1., 0.02)))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  model.add(layers.Conv3DTranspose(1, [4, 4, 4], strides=(2, 2, 2), \n",
        "                                   padding='same', use_bias=False, \n",
        "                                   activation='tanh'))\n",
        "  assert model.output_shape == (None, 16, 80, 80, 1)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def make_discriminator_model(output_units = 1):\n",
        "  df_dim = 32 # Dimension of discrim filters in first conv layer\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv3D(df_dim, (4, 4, 4), strides=(2, 2, 2), \n",
        "                          input_shape=[16, 80, 80, 1],  \n",
        "                          activation=tf.keras.layers.LeakyReLU(),\n",
        "                          padding='same'))\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*2, (4, 4, 4), strides=(2, 2, 2), \n",
        "                          padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*4, (4, 4, 4), strides=(2, 2, 2), \n",
        "                          padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  \n",
        "  model.add(layers.Conv3D(df_dim*8, (4, 4, 4), strides=(2, 2, 2), \n",
        "                          padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(layers.Dense(output_units, \n",
        "            kernel_initializer=tf.random_normal_initializer(stddev=0.02), \n",
        "            input_dim=32))\n",
        "\n",
        "  return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "cp19QaVekyet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils function:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# block of code that generates settlements made of stone\n",
        "def generate_stone(generator, noise_dim, structureCount):\n",
        "\n",
        "  predictions = generator(tf.random.normal([structureCount, noise_dim]), \n",
        "                          training=False)\n",
        "  predictions = predictions.numpy()\n",
        "  predictions.shape = [structureCount, 16, 80, 80]\n",
        "\n",
        "  for i in range(structureCount):\n",
        "    structure = predictions[i, :, :, :]\n",
        "    structure = np.where(structure >= 0, 1, 0)\n",
        "    print(\"generated %s\" % str(structure.shape))\n",
        "    print(\"number of 0 blocks\")\n",
        "    print(np.count_nonzero(structure == 0))\n",
        "    print(\"number of 1 blocks\")\n",
        "    print(np.count_nonzero(structure == 1))\n",
        "    exportSchematic = SchematicFile(shape=structure.shape)\n",
        "    exportSchematic.blocks = structure\n",
        "    exportSchematic.save(folder + str(i) + \".schematic\")\n",
        "    print(\"exported to \" + \"data/generatedExample.schematic\")\n",
        "    print(np.unique(structure.flatten()))\n",
        "\n",
        "\n",
        "# block of code that generates settlements made of different materials\n",
        "\n",
        "# function that converts every value obtained from the models into its nearest\n",
        "# in the allowed values (FILTER1)\n",
        "def nearest_value(X, values):\n",
        "  for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "      for k in range(X.shape[2]):\n",
        "        X[i][j][k] = values[(np.abs(values-X[i][j][k])).argmin()]\n",
        "  return X\n",
        "\n",
        "# same function as the previous, but with encoding\n",
        "def nearest_enc_value(X, values, filter):\n",
        "  for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "      for k in range(X.shape[2]):\n",
        "        X[i][j][k] = filter[values[(np.abs(values-X[i][j][k])).argmin()]]\n",
        "  return X\n",
        "\n",
        "\n",
        "def generate_materials(generator, noise_dim, structureCount):\n",
        "  # Filter of materials that I want to keep\n",
        "  FILTER1 = np.array([0,4,5,8,17,20,43,50,61,64,67,85,98,109,139], dtype=int)\n",
        "  #enc_FILTER1 = np.array(range(FILTER1.shape[0]))\n",
        "\n",
        "  predictions = generator(tf.random.normal([structureCount, noise_dim]), \n",
        "                          training=False)\n",
        "  #predictions = predictions * max(enc_FILTER1)/2 + max(enc_FILTER1)/2\n",
        "  predictions = predictions * max(FILTER1)/2 + max(FILTER1)/2\n",
        "  predictions = predictions.numpy()\n",
        "  predictions.shape = [structureCount, 16, 80, 80]\n",
        "\n",
        "  for i in range(structureCount):\n",
        "    structure = predictions[i, :, :, :]\n",
        "    #structure = nearest_enc_value(structure, enc_FILTER1, FILTER1)\n",
        "    structure = nearest_value(structure, FILTER1)\n",
        "    print(\"generated %s\" % str(structure.shape))\n",
        "    exportSchematic = SchematicFile(shape=structure.shape)\n",
        "    exportSchematic.blocks = structure\n",
        "    exportSchematic.save(folder + str(i) + \".schematic\")\n",
        "    print(\"exported to \" + \"data/generatedExample.schematic\")\n",
        "    print(np.unique(structure.flatten()))\n",
        "\n",
        "# number of structure to generate\n",
        "def summarise_performance(mode, generator, noise_dim, folder, epoch):\n",
        "\n",
        "  if mode == 'stone':\n",
        "    predictions = generator(tf.random.normal([1, noise_dim]), training=False)\n",
        "    predictions = predictions.numpy()\n",
        "    predictions.shape = [1, 16, 80, 80]\n",
        "\n",
        "    structure = predictions[0, :, :, :]\n",
        "    structure = np.where(structure >= 0, 1, 0)\n",
        "    exportSchematic = SchematicFile(shape=structure.shape)\n",
        "    exportSchematic.blocks = structure\n",
        "    exportSchematic.save(folder + str(epoch) + \".schematic\")\n",
        "\n",
        "  else:\n",
        "    # Filter of materials that I want to keep\n",
        "    FILTER1 = np.array([0,4,5,8,17,20,43,50,61,64,67,85,98,109,139], dtype=int)\n",
        "    #enc_FILTER1 = np.array(range(FILTER1.shape[0]))\n",
        "\n",
        "    predictions = generator(tf.random.normal([1, noise_dim]), training=False)\n",
        "    #predictions = predictions * max(enc_FILTER1)/2 + max(enc_FILTER1)/2\n",
        "    predictions = predictions * max(FILTER1)/2 + max(FILTER1)/2\n",
        "    predictions = predictions.numpy()\n",
        "    predictions.shape = [1, 16, 80, 80]\n",
        "\n",
        "    structure = predictions[0, :, :, :]\n",
        "    #structure = nearest_enc_value(structure, enc_FILTER1, FILTER1)\n",
        "    structure = nearest_value(structure, FILTER1)\n",
        "    exportSchematic = SchematicFile(shape=structure.shape)\n",
        "    exportSchematic.blocks = structure\n",
        "    exportSchematic.save(folder + str(epoch) + \".schematic\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "7b_oPUJOqRlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the training procedure. 3D-WGAN or 3D-DCGAN:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3D-WGAN"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#3D-WGAN\n",
        "\n",
        "BUFFER_SIZE = 2500\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1000\n",
        "noise_dim = 200\n",
        "mode = 'stone' # or 'materials'\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# this method computes the gradient penalty applied to the discriminator loss.\n",
        "# this process works with an interpolated image between the real and the fake\n",
        "# image. As defined in \"Improved Training of Wasserstein GANs\":\n",
        "def gradient_penalty(batch_size, discriminator, real_images, fake_images):\n",
        "  # calculating the interpolated image\n",
        "  alpha = tf.random.normal([batch_size, 1, 1, 1, 1], 0.0, 1.0)\n",
        "  diff = fake_images - real_images\n",
        "  interpolated = real_images + alpha * diff\n",
        "\n",
        "  with tf.GradientTape() as gp_tape:\n",
        "      gp_tape.watch(interpolated)\n",
        "      # Getting the discriminator output for the interpolated image\n",
        "      pred = discriminator(interpolated, training=True)\n",
        "\n",
        "  # getting the interpolated image gradient\n",
        "  grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "  # calculating the norm of the gradient\n",
        "  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "  gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "  return gp\n",
        "\n",
        "# discriminator loss function\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = tf.reduce_mean(real_output)\n",
        "    fake_loss = tf.reduce_mean(fake_output)\n",
        "    return fake_loss - real_loss\n",
        "\n",
        "# generator loss function\n",
        "def generator_loss(fake_output):\n",
        "    return -tf.reduce_mean(fake_output)\n",
        "\n",
        "# defining the generator and discriminator optimisers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "# setting the model checkpoint in order to periodically save models at different\n",
        "# stages of the training\n",
        "checkpoint_dir = '/content/gdrive/MyDrive/FProject/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# function that allows to load a pre-trained model if there are any\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)).expect_partial()\n",
        "\n",
        "# losses history\n",
        "disc_hist, gen_hist = list(), list()\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "  if isinstance(images, tuple):\n",
        "      images = images[0]\n",
        "\n",
        "  # getting the batch size\n",
        "  batch_size = tf.shape(images)[0]\n",
        "\n",
        "  # the original paper recomends to train the discriminator 5 times but because \n",
        "  # this task is time consuming, we will train the discriminator 3 times\n",
        "\n",
        "  # train the discriminator\n",
        "  for i in range(3):\n",
        "      # getting the random vectors that are defined in the latent space of\n",
        "      # dimension 'noiese_dim'\n",
        "      latent_vectors = tf.random.normal(shape=(batch_size, noise_dim))\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          # obtaining the generator fake image\n",
        "          generated_images = generator(latent_vectors, training=True)\n",
        "          # getting the discriminator result on the fake image\n",
        "          fake_output = discriminator(generated_images, training=True)\n",
        "          # getting the discriminator result on the original image\n",
        "          real_output = discriminator(images, training=True)\n",
        "\n",
        "          # it computes the discriminator loss using the fake and real\n",
        "          # discriminator output\n",
        "          disc_cost = discriminator_loss(real_output, fake_output)\n",
        "          # gradient penalty computation\n",
        "          gp = gradient_penalty(batch_size, discriminator, images\n",
        "                                , generated_images)\n",
        "          # summing the old discriminator loss (disc_cost) to \n",
        "          # the gradient penalty that is multiplied by labda which is 10\n",
        "          disc_loss = disc_cost + gp * 10.0\n",
        "\n",
        "      # gradient calculation on the discriminattor loss\n",
        "      disc_gradient = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "      # weights update using the discriminator optimiser\n",
        "      discriminator_optimizer.apply_gradients(\n",
        "          zip(disc_gradient, discriminator.trainable_variables)\n",
        "      )\n",
        "\n",
        "  # train the generator\n",
        "  # getting the random vectors that are defined in the latent space of\n",
        "  # dimension 'noiese_dim'\n",
        "  latent_vectors = tf.random.normal(shape=(batch_size, noise_dim))\n",
        "  with tf.GradientTape() as tape:\n",
        "      # obtaining the generator fake image\n",
        "      generated_images = generator(latent_vectors, training=True)\n",
        "      # getting the discriminator result on the fake image\n",
        "      g_fake_output = discriminator(generated_images, training=True)\n",
        "      \n",
        "      # it computes the generator loss using the discriminator fake output \n",
        "      gen_loss = generator_loss(g_fake_output)\n",
        "\n",
        "  # getting the gradients of gen_loss\n",
        "  gen_gradient = tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  # weights update using the generator optimiser\n",
        "  generator_optimizer.apply_gradients(\n",
        "      zip(gen_gradient, generator.trainable_variables))\n",
        "  \n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # saving the model every 100 epochs and generating an image\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "      summarise_performance(mode, generator, noise_dim, \n",
        "                            checkpoint_dir + \"/\", epoch)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "outputs": [],
      "metadata": {
        "id": "_5XSkcKF3BQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f7a5ed-958b-4449-eaae-266eb099e1ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3D-DCGAN"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 3D-DCGAN\n",
        "BUFFER_SIZE = 2500\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1500\n",
        "noise_dim = 200\n",
        "mode = 'stone'\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# this method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# discriminator loss function\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  return cross_entropy(real_output, fake_output)\n",
        "\n",
        "# generator loss function\n",
        "def generator_loss(fake_output, target):\n",
        "    return cross_entropy(fake_output, target)\n",
        "\n",
        "# defining the generator and discriminator optimisers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0025, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.00005, beta_1=0.5)\n",
        "\n",
        "# setting the model checkpoint in order to periodically save models at different\n",
        "# stages of the training\n",
        "checkpoint_dir = '/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "# losses history\n",
        "disc_hist1, disc_hist2, gen_hist = list(), list(), list()\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    # getting the random vectors that are defined in the latent space of\n",
        "    # dimension 'noiese_dim'\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    latent_vectors = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "    # train Discriminator with real labels\n",
        "    with tf.GradientTape() as disc_tape1:\n",
        "        # obtaining the generator fake image\n",
        "        generated_images = generator(latent_vectors, training=True)\n",
        "        # getting the discriminator result on the real image\n",
        "        real_output = discriminator(images, training=True)\n",
        "        # discriminator target\n",
        "        real_targets = tf.ones_like(real_output)\n",
        "        \n",
        "        # it computes the discriminator loss using the real and\n",
        "        # the exptected discriminator output\n",
        "        disc_loss1 = discriminator_loss(real_targets, real_output)\n",
        "        \n",
        "    # gradient calculation on the discriminattor loss    \n",
        "    gradients_of_disc1 = disc_tape1.gradient(disc_loss1, \n",
        "                                             discriminator.trainable_variables)\n",
        "    \n",
        "    # weights update using the discriminator optimiser   \n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc1,\\\n",
        "                                            discriminator.trainable_variables))\n",
        "    \n",
        "    # train Discriminator with fake labels\n",
        "    with tf.GradientTape() as disc_tape2:\n",
        "        # getting the discriminator result on the fake image\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        # discriminator target\n",
        "        fake_targets = tf.zeros_like(fake_output)\n",
        "\n",
        "        # it computes the discriminator loss using the fake and\n",
        "        # the exptected discriminator output\n",
        "        disc_loss2 = discriminator_loss(fake_targets, fake_output)\n",
        "\n",
        "    # gradient calculation on the discriminattor loss\n",
        "    gradients_of_disc2 = disc_tape2.gradient(disc_loss2, \n",
        "                                             discriminator.trainable_variables)\n",
        "    \n",
        "    \n",
        "    # weights update using the generator optimiser        \n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc2,\\\n",
        "    discriminator.trainable_variables))\n",
        "    \n",
        "    # train Generator with real labels\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        # obtaining the generator fake image\n",
        "        generated_images = generator(latent_vectors, training=True)\n",
        "        # getting the discriminator result on the fake image\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        # generator target\n",
        "        real_targets = tf.ones_like(fake_output)\n",
        "\n",
        "        # it computes the generator loss using the fake and\n",
        "        # the exptected generator output\n",
        "        gen_loss = generator_loss(fake_output, real_targets)\n",
        "\n",
        "    # getting the gradients of the generators    \n",
        "    gradients_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    \n",
        "    # weights update using the generator optimiser\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_gen,\\\n",
        "    generator.trainable_variables))\n",
        "\n",
        "    disc_hist1.append(disc_loss1)\n",
        "    disc_hist2.append(disc_loss2)\n",
        "    gen_hist.append(gen_loss)\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    print()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # saving the model every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "      summarise_performance(mode, generator, noise_dim, \n",
        "                            checkpoint_dir + \"/\", epoch)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini optional augmentation demo:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# funtions for the augmentation\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    image_size = tf.shape(x)[1:4]\n",
        "    shift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
        "    translation_x = tf.random.uniform([batch_size, 1], -shift[0], shift[0] + 1, \n",
        "                                      dtype=tf.int32)\n",
        "    translation_y = tf.random.uniform([batch_size, 1], -shift[1], shift[1] + 1, \n",
        "                                      dtype=tf.int32)\n",
        "    translation_z = tf.random.uniform([batch_size, 1], -shift[2], shift[2] + 1, \n",
        "                                      dtype=tf.int32)\n",
        "    grid_x = tf.clip_by_value(tf.expand_dims(tf.range(image_size[0],\n",
        "                  dtype=tf.int32), 0) + translation_x + 1, 0, image_size[0] + 1)\n",
        "    grid_y = tf.clip_by_value(tf.expand_dims(tf.range(image_size[1], \n",
        "                  dtype=tf.int32), 0) + translation_y + 1, 0, image_size[1] + 1)\n",
        "    grid_z = tf.clip_by_value(tf.expand_dims(tf.range(image_size[2], \n",
        "                  dtype=tf.int32), 0) + translation_z + 1, 0, image_size[2] + 1)\n",
        "    x = tf.gather_nd(tf.pad(x, [[0, 0], [1, 1], [0, 0], [0, 0], [0, 0]]), \n",
        "                     tf.expand_dims(grid_x, -1), batch_dims=1)\n",
        "    x = tf.transpose(tf.gather_nd(tf.pad(tf.transpose(x, [0, 2, 1, 3, 4]), \n",
        "                    [[0, 0], [1, 1], [0, 0], [0, 0], [0, 0]]), \n",
        "                    tf.expand_dims(grid_y, -1), batch_dims=1), [0, 2, 1, 3, 4])\n",
        "    x = tf.transpose(tf.gather_nd(tf.pad(tf.transpose(x, [0, 2, 1, 3, 4]), \n",
        "                    [[0, 0], [1, 1], [0, 0], [0, 0], [0, 0]]), \n",
        "                    tf.expand_dims(grid_z, -1), batch_dims=1), [0, 2, 1, 3, 4])\n",
        "    return x\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    image_size = tf.shape(x)[1:4]\n",
        "    cutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
        "    offset_x = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[0] + \n",
        "                                 (1 - cutout_size[0] % 2), dtype=tf.int32)\n",
        "    offset_y = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[1] + \n",
        "                                 (1 - cutout_size[1] % 2), dtype=tf.int32)\n",
        "    offset_z = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[2] + \n",
        "                                 (1 - cutout_size[2] % 2), dtype=tf.int32)\n",
        "    grid_batch, grid_x, grid_y, grid_z = tf.meshgrid(tf.range(batch_size, \n",
        "                                         dtype=tf.int32), \n",
        "                                         tf.range(cutout_size[0], dtype=tf.int32), \n",
        "                                         tf.range(cutout_size[1], dtype=tf.int32), \n",
        "                                         tf.range(cutout_size[2], dtype=tf.int32), \n",
        "                                         indexing='ij')\n",
        "    cutout_grid = tf.stack([grid_batch, grid_x + offset_x - cutout_size[0] // 2, \n",
        "                            grid_y + offset_y - cutout_size[1] // 2, grid_z + \n",
        "                            offset_z - cutout_size[2] // 2], axis=-1)\n",
        "    mask_shape = tf.stack([batch_size, image_size[0], \n",
        "                           image_size[1, image_size[2]]])\n",
        "    cutout_grid = tf.maximum(cutout_grid, 0)\n",
        "    cutout_grid = tf.minimum(cutout_grid, tf.reshape(mask_shape - 1, \n",
        "                                                     [1, 1, 1, 1, 3]))\n",
        "    mask = tf.maximum(1 - tf.scatter_nd(cutout_grid, tf.ones([batch_size, \n",
        "                                        cutout_size[0], cutout_size[1], \n",
        "                                        cutout_size[2]], dtype=tf.float32),\n",
        "                                        mask_shape), 0)\n",
        "    x = x * tf.expand_dims(mask, axis=4)\n",
        "    return x\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "'translation': [rand_translation]#,\n",
        "'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "def DiffAugment(x, policy=''):\n",
        "    if policy:\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "    return x"
      ],
      "outputs": [],
      "metadata": {
        "id": "I8kk7Olzj3SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentation demo"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 3D-DCGAN AUGMENTATION\n",
        "\n",
        "policy = ''\n",
        "BUFFER_SIZE = 2500\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "noise_dim = 200\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# this method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# discriminator loss function\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  return cross_entropy(real_output, fake_output)\n",
        "\n",
        "# generator loss function\n",
        "def generator_loss(fake_output, target):\n",
        "    return cross_entropy(fake_output, target)\n",
        "\n",
        "# defining the generator and discriminator optimisers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0025, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.00005, beta_1=0.5)\n",
        "\n",
        "# setting the model checkpoint in order to periodically save models at different\n",
        "# stages of the training\n",
        "checkpoint_dir = '/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    # getting the random vectors that are defined in the latent space of\n",
        "    # dimension 'noiese_dim'\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    latent_vectors = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "    # train Discriminator with real labels\n",
        "    with tf.GradientTape() as disc_tape1:\n",
        "        # obtaining the generator fake image\n",
        "        generated_images = generator(latent_vectors, training=True)\n",
        "        # augmentation\n",
        "        generated_images = DiffAugment(generated_images, policy=policy, \n",
        "                                       channels_first=True)\n",
        "        # getting the discriminator result on the real image with augmentation\n",
        "        real_output = discriminator(DiffAugment(images, policy=policy, \n",
        "                                            channels_first=True), training=True)\n",
        "        # discriminator target\n",
        "        real_targets = tf.ones_like(real_output)\n",
        "        \n",
        "        # it computes the discriminator loss using the real and\n",
        "        # the exptected discriminator output. it is applied augmentation\n",
        "        disc_loss1 = discriminator_loss(DiffAugment(real_targets, policy=policy, \n",
        "                                              channels_first=True), real_output)\n",
        "        \n",
        "    # gradient calculation on the discriminattor loss    \n",
        "    gradients_of_disc1 = disc_tape1.gradient(disc_loss1, \n",
        "                                             discriminator.trainable_variables)\n",
        "    \n",
        "    # weights update using the discriminator optimiser   \n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc1,\\\n",
        "                                            discriminator.trainable_variables))\n",
        "    \n",
        "    # train Discriminator with fake labels\n",
        "    with tf.GradientTape() as disc_tape2:\n",
        "        # getting the discriminator result on the fake image\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        # discriminator target\n",
        "        fake_targets = tf.zeros_like(fake_output)\n",
        "\n",
        "        # it computes the discriminator loss using the fake and\n",
        "        # the exptected discriminator output. it is applied augmentation\n",
        "        disc_loss2 = discriminator_loss(DiffAugment(fake_targets, policy=policy, \n",
        "                                              channels_first=True), fake_output)\n",
        "\n",
        "    # gradient calculation on the discriminattor loss\n",
        "    gradients_of_disc2 = disc_tape2.gradient(disc_loss2, \n",
        "                                             discriminator.trainable_variables)\n",
        "    \n",
        "    \n",
        "    # weights update using the generator optimiser        \n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc2,\\\n",
        "    discriminator.trainable_variables))\n",
        "    \n",
        "    # train Generator with real labels\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        # obtaining the generator fake image\n",
        "        generated_images = generator(latent_vectors, training=True)\n",
        "        # augmentation\n",
        "        generated_images = DiffAugment(generated_images, policy=policy, \n",
        "                                       channels_first=True)\n",
        "        # getting the discriminator result on the fake image\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        # generator target\n",
        "        real_targets = tf.ones_like(fake_output)\n",
        "\n",
        "        # it computes the generator loss using the fake and\n",
        "        # the exptected generator output\n",
        "        gen_loss = generator_loss(DiffAugment(fake_output, real_targets, \n",
        "                                              policy=policy, channels_first=True), \n",
        "                                  real_targets)\n",
        "\n",
        "    # getting the gradients of the generators    \n",
        "    gradients_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    \n",
        "    # weights update using the generator optimiser\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_gen,\\\n",
        "    generator.trainable_variables))    \n",
        "\n",
        "# function that simulates the augmentation generating \n",
        "# a sample to show the augmented images\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    print()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      structure = image_batch[0].numpy()[:, :, :, 0]\n",
        "      structure = np.where(structure > 0, 1, 0)\n",
        "      print(structure.shape)\n",
        "      exportSchematic = SchematicFile(shape=structure.shape)\n",
        "      exportSchematic.blocks = structure\n",
        "      exportSchematic.save(folder + \"0\" + \".schematic\")\n",
        "\n",
        "      prova = DiffAugment(image_batch, policy='cutout')\n",
        "      structure = prova[0].numpy()[:, :, :, 0]\n",
        "      print(structure.shape)\n",
        "      structure = np.where(structure > 0, 1, 0)\n",
        "      exportSchematic = SchematicFile(shape=structure.shape)\n",
        "      exportSchematic.blocks = structure\n",
        "      exportSchematic.save(folder + \"1\" + \".schematic\")\n",
        "      break\n",
        "\n",
        "train(train_dataset, EPOCHS)"
      ],
      "outputs": [],
      "metadata": {
        "id": "y0ZbM0gyZ7sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to load the trained models:\n",
        "1. Execute the cell of the architecure of the generator and discriminator that is wanted to be loaded\n",
        "2. Execute the following cell"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# block of code that loads the model checkpoints that are in the folder\n",
        "# defined in checkpoint_dir\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "checkpoint_dir = '/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Load our pre-trained model\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)).expect_partial()\n",
        "\n",
        "print(generator.summary())\n",
        "print(discriminator.summary())\n",
        "\n",
        "# via this function the settlements will be generated and saved in the current directory\n",
        "#generate_stone(generator, noise_dim=200, structureCount=3)\n",
        "generate_stone(generator, noise_dim=100, structureCount=3)"
      ],
      "outputs": [],
      "metadata": {
        "id": "W8an_6J9cXQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose what kind of models you want to evaluate"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0-1 models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import SchematicTools\n",
        "from schematic import SchematicFile\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_dataset():\n",
        "    l = []\n",
        "    for file in os.listdir(folder + \"dataset808016/\"):\n",
        "        settlement = SchematicTools.simplify(SchematicTools.loadArea(folder + \"dataset808016/\" + file))\n",
        "        l.append(settlement)\n",
        "    x = np.array(l)\n",
        "    return (x)\n",
        "\n",
        "# generating EMD values:\n",
        "def EMD(X, settlement):\n",
        "  EMDs = np.zeros(2500)\n",
        "  i = 0\n",
        "  for x in X:\n",
        "    EMDs[i] = np.sqrt(np.mean(np.square(np.abs(np.cumsum(x) - np.cumsum(settlement)))))\n",
        "    i = i + 1\n",
        "  return EMDs\n",
        "\n",
        "X = load_dataset()\n",
        "X = X * 1\n",
        "\n",
        "directory = '/content/trial01/'\n",
        "# computing the EMDs between the settlement and the data set\n",
        "for filename in os.listdir(directory):\n",
        "    settlement = settlement = SchematicTools.simplify(SchematicTools.loadArea(directory + filename))\n",
        "    settlement = settlement * 1\n",
        "    EMDs = EMD(X, settlement)\n",
        "    print(filename)\n",
        "    print(\"EMD distance to the nearest neighbour:\")\n",
        "    print(min(EMDs))\n",
        "    print(\"EMD distance to the nearest neighbour:\")\n",
        "    print(np.mean(EMDs))\n",
        "    print(\"\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaiJTV9W9Y1l",
        "outputId": "c1bb6703-0abe-4359-fd4d-0b79a78a0ab0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## all materials models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import SchematicTools\n",
        "from schematic import SchematicFile\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_dataset():\n",
        "    l = []\n",
        "    for file in os.listdir(folder + \"dataset808016/\"):\n",
        "        settlement = SchematicTools.simplify2(SchematicTools.loadArea(folder + \"dataset808016/\" + file))\n",
        "        l.append(settlement)\n",
        "    x = np.array(l)\n",
        "    return (x)\n",
        "\n",
        "# generating EMD values:\n",
        "def EMD(X, settlement):\n",
        "  EMDs = np.zeros(2500)\n",
        "  i = 0\n",
        "  for x in X:\n",
        "    EMDs[i] = np.sqrt(np.mean(np.square(np.abs(np.cumsum(x) - np.cumsum(settlement)))))\n",
        "    i = i + 1\n",
        "  return EMDs\n",
        "\n",
        "X = load_dataset()\n",
        "\n",
        "directory = '/content/trialmaterials/'\n",
        "# computing the EMDs between the settlement and the data set\n",
        "for filename in os.listdir(directory):\n",
        "    settlement = settlement = SchematicTools.simplify2(SchematicTools.loadArea(directory + filename))\n",
        "    settlement = settlement * 1\n",
        "    EMDs = EMD(X, settlement)\n",
        "    print(filename)\n",
        "    print(\"EMD distance to the nearest neighbour:\")\n",
        "    print(min(EMDs))\n",
        "    print(\"EMD distance to the nearest neighbour:\")\n",
        "    print(np.mean(EMDs))\n",
        "    print(\"\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgNfdtAvDBpY",
        "outputId": "eb13f729-b291-4655-9f4c-138844ad2fb6"
      }
    }
  ]
}